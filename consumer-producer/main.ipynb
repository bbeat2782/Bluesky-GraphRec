{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('records',)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from factorize import factorize\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import faiss\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Create/connect to DuckDB database\n",
    "con = duckdb.connect('../random_tests/scan_results.duckdb')\n",
    "con.execute(\"SHOW TABLES\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Producers (Users with >= 30 followers) before training cutoff date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>producer_did</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>did:plc:cm4bwax4evxmkiuwxvvkvlmx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>did:plc:cnujroalnuchtfjozznzxfm3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did:plc:vm7wifn25awqm2zrury5pudg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>did:plc:ylmbufmy5btcigjk27oup4zl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>did:plc:zbrd6f4ykvwgftv66lv2ozjf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37187</th>\n",
       "      <td>did:plc:jikpepa6gimefucl7pebrely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37188</th>\n",
       "      <td>did:plc:mrfolvuqzuv54n6wlljyoox5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37189</th>\n",
       "      <td>did:plc:qmrd2hmucin2ucnfb36hry3y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37190</th>\n",
       "      <td>did:plc:4z7vjxv47t6gk4yknfxiaiwb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37191</th>\n",
       "      <td>did:plc:nybik5votimmgcwfy7nnfdgc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37192 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           producer_did\n",
       "0      did:plc:cm4bwax4evxmkiuwxvvkvlmx\n",
       "1      did:plc:cnujroalnuchtfjozznzxfm3\n",
       "2      did:plc:vm7wifn25awqm2zrury5pudg\n",
       "3      did:plc:ylmbufmy5btcigjk27oup4zl\n",
       "4      did:plc:zbrd6f4ykvwgftv66lv2ozjf\n",
       "...                                 ...\n",
       "37187  did:plc:jikpepa6gimefucl7pebrely\n",
       "37188  did:plc:mrfolvuqzuv54n6wlljyoox5\n",
       "37189  did:plc:qmrd2hmucin2ucnfb36hry3y\n",
       "37190  did:plc:4z7vjxv47t6gk4yknfxiaiwb\n",
       "37191  did:plc:nybik5votimmgcwfy7nnfdgc\n",
       "\n",
       "[37192 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_producer_df = con.execute(\"\"\"\n",
    "WITH producers AS (\n",
    "    SELECT \n",
    "        json_extract_string(record, '$.subject') as producer_did\n",
    "    FROM records \n",
    "    WHERE collection = 'app.bsky.graph.follow'\n",
    "    AND createdAt < '2023-06-15'  -- before training cutoff date\n",
    "    GROUP BY json_extract_string(record, '$.subject')\n",
    "    HAVING COUNT(*) >= 30\n",
    ")\n",
    "SELECT producer_did\n",
    "FROM producers\n",
    "\"\"\").fetchdf()\n",
    "train_producer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Training Edges (Consumer-Producer Follows Bipartite Graph)\n",
    "Excludes: \n",
    "- Follows after training cutoff date\n",
    "- Follows from producers with less than 30 followers\n",
    "- Posts before 2023-03-01 (Start of the network)\n",
    "- Likes before 2023-06-15 (Training period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumer_did</th>\n",
       "      <th>producer_did</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>did:plc:7hxhbhphfselzxjxhrxfykzr</td>\n",
       "      <td>did:plc:nvog7rczakwzh5ckxnjnwqdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>did:plc:7hxhbhphfselzxjxhrxfykzr</td>\n",
       "      <td>did:plc:ohvstchboonnmbplvwkl33ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did:plc:7hxhbhphfselzxjxhrxfykzr</td>\n",
       "      <td>did:plc:cdgrfvzrwkcx6o6s4ek47k4o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>did:plc:7hxhbhphfselzxjxhrxfykzr</td>\n",
       "      <td>did:plc:sdxk3j4fv3nshpos7624mjjv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>did:plc:7hxhbhphfselzxjxhrxfykzr</td>\n",
       "      <td>did:plc:f5xkhushrnb4snbxuohamy4k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781719</th>\n",
       "      <td>did:plc:p4ar3z4uwnmuvwlybq7kgolj</td>\n",
       "      <td>did:plc:di3xrpx4l3bsgmktdirfsxcv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781720</th>\n",
       "      <td>did:plc:p4ar3z4uwnmuvwlybq7kgolj</td>\n",
       "      <td>did:plc:4lf7xpfobie7l4hct6coqojd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781721</th>\n",
       "      <td>did:plc:p4ar3z4uwnmuvwlybq7kgolj</td>\n",
       "      <td>did:plc:2fsrsv2z3kvwibizd7nyjpk4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781722</th>\n",
       "      <td>did:plc:p4ar3z4uwnmuvwlybq7kgolj</td>\n",
       "      <td>did:plc:uoeyj3ozza6ry7lhvnk6urwp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781723</th>\n",
       "      <td>did:plc:p4ar3z4uwnmuvwlybq7kgolj</td>\n",
       "      <td>did:plc:ig24mxpbd2grrmqmsxcxoxl6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5781724 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             consumer_did                      producer_did\n",
       "0        did:plc:7hxhbhphfselzxjxhrxfykzr  did:plc:nvog7rczakwzh5ckxnjnwqdd\n",
       "1        did:plc:7hxhbhphfselzxjxhrxfykzr  did:plc:ohvstchboonnmbplvwkl33ko\n",
       "2        did:plc:7hxhbhphfselzxjxhrxfykzr  did:plc:cdgrfvzrwkcx6o6s4ek47k4o\n",
       "3        did:plc:7hxhbhphfselzxjxhrxfykzr  did:plc:sdxk3j4fv3nshpos7624mjjv\n",
       "4        did:plc:7hxhbhphfselzxjxhrxfykzr  did:plc:f5xkhushrnb4snbxuohamy4k\n",
       "...                                   ...                               ...\n",
       "5781719  did:plc:p4ar3z4uwnmuvwlybq7kgolj  did:plc:di3xrpx4l3bsgmktdirfsxcv\n",
       "5781720  did:plc:p4ar3z4uwnmuvwlybq7kgolj  did:plc:4lf7xpfobie7l4hct6coqojd\n",
       "5781721  did:plc:p4ar3z4uwnmuvwlybq7kgolj  did:plc:2fsrsv2z3kvwibizd7nyjpk4\n",
       "5781722  did:plc:p4ar3z4uwnmuvwlybq7kgolj  did:plc:uoeyj3ozza6ry7lhvnk6urwp\n",
       "5781723  did:plc:p4ar3z4uwnmuvwlybq7kgolj  did:plc:ig24mxpbd2grrmqmsxcxoxl6\n",
       "\n",
       "[5781724 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the edges (consumer-producer relationships)\n",
    "train_edges_df = con.execute(\"\"\"\n",
    "SELECT \n",
    "    repo as consumer_did,\n",
    "    json_extract_string(record, '$.subject') as producer_did\n",
    "FROM records\n",
    "WHERE \n",
    "    collection = 'app.bsky.graph.follow'\n",
    "    AND json_extract_string(record, '$.subject') IN (SELECT producer_did FROM train_producer_df)\n",
    "\"\"\").fetchdf()\n",
    "train_edges_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persistent Consumer/Producer ID Mappings. If you want to start over from scratch, delete the mappings files and rerun the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mappings unchanged, safe to use existing post embeddings.\n",
      "Matrix shape: (132728, 37192)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import hashlib\n",
    "\n",
    "def get_mapping_hash(mapping):\n",
    "    \"\"\"\n",
    "    Create a deterministic hash of a mapping dictionary.\n",
    "    \"\"\"\n",
    "    # Convert mapping to a sorted list of tuples to ensure consistent ordering\n",
    "    sorted_items = sorted(mapping.items())\n",
    "    # Convert to string and encode to bytes\n",
    "    mapping_str = json.dumps(sorted_items)\n",
    "    return hashlib.sha256(mapping_str.encode()).hexdigest()\n",
    "\n",
    "def load_mapping(mapping_file):\n",
    "    \"\"\"\n",
    "    Load a mapping from a JSON file. If the file doesn't exist, return an empty dict.\n",
    "    \"\"\"\n",
    "    if os.path.exists(mapping_file):\n",
    "        with open(mapping_file, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "    else:\n",
    "        mapping = {}\n",
    "    return mapping\n",
    "\n",
    "def update_mapping(mapping, new_items):\n",
    "    \"\"\"\n",
    "    Update the mapping with new items. New items are appended by assigning \n",
    "    them an index equal to the current length of the mapping.\n",
    "    \"\"\"\n",
    "    for item in new_items:\n",
    "        if item not in mapping:\n",
    "            mapping[item] = len(mapping)\n",
    "    return mapping\n",
    "\n",
    "# File paths for persistent mappings\n",
    "consumer_mapping_file = 'consumer_mapping.json'\n",
    "producer_mapping_file = 'producer_mapping.json'\n",
    "hash_file = 'mappings_hash.json'\n",
    "\n",
    "# Load existing mappings (or create new ones if they don't exist)\n",
    "consumer_to_idx = load_mapping(consumer_mapping_file)\n",
    "producer_to_idx = load_mapping(producer_mapping_file)\n",
    "\n",
    "# Store original hashes\n",
    "original_hashes = {\n",
    "    'consumer': get_mapping_hash(consumer_to_idx),\n",
    "    'producer': get_mapping_hash(producer_to_idx)\n",
    "}\n",
    "\n",
    "# Get new DIDs from the current training data\n",
    "new_consumers = train_edges_df['consumer_did'].unique().tolist()\n",
    "new_producers = train_producer_df['producer_did'].unique().tolist()\n",
    "\n",
    "# Update the mappings with any new DIDs\n",
    "consumer_to_idx = update_mapping(consumer_to_idx, new_consumers)\n",
    "producer_to_idx = update_mapping(producer_to_idx, new_producers)\n",
    "\n",
    "# Get new hashes\n",
    "new_hashes = {\n",
    "    'consumer': get_mapping_hash(consumer_to_idx),\n",
    "    'producer': get_mapping_hash(producer_to_idx)\n",
    "}\n",
    "\n",
    "# Check if mappings changed\n",
    "mappings_changed = (original_hashes != new_hashes)\n",
    "\n",
    "if mappings_changed:\n",
    "    print(\"Warning: Mappings have changed! You should recompute post embeddings.\")\n",
    "    # Save the updated mappings to disk\n",
    "    with open(consumer_mapping_file, 'w') as f:\n",
    "        json.dump(consumer_to_idx, f)\n",
    "    with open(producer_mapping_file, 'w') as f:\n",
    "        json.dump(producer_to_idx, f)\n",
    "    # Save the new hashes\n",
    "    with open(hash_file, 'w') as f:\n",
    "        json.dump(new_hashes, f)\n",
    "else:\n",
    "    print(\"Mappings unchanged, safe to use existing post embeddings.\")\n",
    "\n",
    "# Create sparse matrix in COO format; each edge has weight 1\n",
    "rows = [consumer_to_idx[consumer] for consumer in train_edges_df['consumer_did']]\n",
    "cols = [producer_to_idx[producer] for producer in train_edges_df['producer_did']]\n",
    "data = np.ones(len(rows))\n",
    "\n",
    "# Build the sparse matrix (then convert to CSR format for efficient multiplication)\n",
    "matrix = sp.coo_matrix(\n",
    "    (data, (rows, cols)),\n",
    "    shape=(len(consumer_to_idx), len(producer_to_idx))\n",
    ")\n",
    "\n",
    "print(\"Matrix shape:\", matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it because for SVD I did L2 norm? Also look into deterministic and handling of new consumers/producers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer embeddings shape: (132728, 64)\n",
      "Average consumer embedding L2 norm: 1.000\n",
      "Average producer affinity: 0.490\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)  # IMPORTANT: temporary solution for deterministic results. Need this so that consumer_embeddings stays the same across runs.\n",
    "producer_community_affinities, consumer_embeddings, kmeans_cluster_centers = factorize(\n",
    "    matrix, \n",
    "    algorithm='svd',\n",
    "    n_components=64,\n",
    "    n_clusters=100,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Print some stats\n",
    "print(f\"Consumer embeddings shape: {consumer_embeddings.shape}\")\n",
    "print(f\"Average consumer embedding L2 norm: {np.mean([np.linalg.norm(emb) for emb in consumer_embeddings]):.3f}\")\n",
    "print(f\"Average producer affinity: {producer_community_affinities.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.linalg.norm(emb) for emb in consumer_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average dimensions needed for 90% of magnitude: 40.9\n",
      "Median dimensions needed for 90% of magnitude: 41.0\n",
      "25th percentile dimensions: 39.0\n",
      "75th percentile dimensions: 43.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate how many dimensions are needed to reach 90% of total magnitude for each consumer\n",
    "consumer_magnitudes = np.sort(np.abs(consumer_embeddings), axis=1)[:, ::-1]  # Sort each row in descending order\n",
    "consumer_cumsum = np.cumsum(consumer_magnitudes, axis=1)\n",
    "consumer_totals = consumer_cumsum[:, -1:]  # Get final sums\n",
    "consumer_cumsum_norm = consumer_cumsum / consumer_totals  # Normalize to get cumulative percentages\n",
    "\n",
    "# Count dimensions needed for 90% per consumer\n",
    "dims_for_90 = np.sum(consumer_cumsum_norm < 0.9, axis=1) + 1\n",
    "print(f\"Average dimensions needed for 90% of magnitude: {dims_for_90.mean():.1f}\")\n",
    "print(f\"Median dimensions needed for 90% of magnitude: {np.median(dims_for_90):.1f}\")\n",
    "print(f\"25th percentile dimensions: {np.percentile(dims_for_90, 25):.1f}\")\n",
    "print(f\"75th percentile dimensions: {np.percentile(dims_for_90, 75):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leftover code; ignore producer_community_affinities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average affinity: 0.490\n",
      "Median affinity: 0.494\n",
      "25th percentile: 0.413\n",
      "75th percentile: 0.573\n",
      "Number of producers with affinity < 0.25: 1022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.42469323, 0.39716443, 0.25889128, ..., 0.2948988 , 0.23110879,\n",
       "       0.81269965])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Average affinity: {producer_community_affinities.mean():.3f}\")\n",
    "print(f\"Median affinity: {np.median(producer_community_affinities):.3f}\")\n",
    "print(f\"25th percentile: {np.percentile(producer_community_affinities, 25):.3f}\")\n",
    "print(f\"75th percentile: {np.percentile(producer_community_affinities, 75):.3f}\")\n",
    "print(f\"Number of producers with affinity < 0.25: {(producer_community_affinities < 0.25).sum()}\")\n",
    "producer_community_affinities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumer_did</th>\n",
       "      <th>post_uri</th>\n",
       "      <th>createdAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>did:plc:bnvfxxa4jri24vhmupgdus7l</td>\n",
       "      <td>at://did:plc:623st67kkthivj4c6icvkqnq/app.bsky...</td>\n",
       "      <td>2023-06-15 01:49:26.986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>did:plc:bnvfxxa4jri24vhmupgdus7l</td>\n",
       "      <td>at://did:plc:vw2smontq2ruzmir67hj4igs/app.bsky...</td>\n",
       "      <td>2023-06-15 07:09:49.695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did:plc:bnvfxxa4jri24vhmupgdus7l</td>\n",
       "      <td>at://did:plc:g3upacmdqfkflmiyvjdmv4wi/app.bsky...</td>\n",
       "      <td>2023-06-15 17:07:22.665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>did:plc:bnvfxxa4jri24vhmupgdus7l</td>\n",
       "      <td>at://did:plc:rfoctmk4guq56gensuermj7q/app.bsky...</td>\n",
       "      <td>2023-06-15 17:08:16.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>did:plc:bnvfxxa4jri24vhmupgdus7l</td>\n",
       "      <td>at://did:plc:36ywnjbmhzu2umjtvuplrvp5/app.bsky...</td>\n",
       "      <td>2023-06-15 17:09:59.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312161</th>\n",
       "      <td>did:plc:znz5mi2kkmbpwqpgqsgtqhjf</td>\n",
       "      <td>at://did:plc:4q5m3qghnw2uf6n2vucmrytc/app.bsky...</td>\n",
       "      <td>2023-06-15 19:06:24.809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312162</th>\n",
       "      <td>did:plc:znz5mi2kkmbpwqpgqsgtqhjf</td>\n",
       "      <td>at://did:plc:tdnmrckaby7w4ikdneqlbdai/app.bsky...</td>\n",
       "      <td>2023-06-15 19:09:55.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312163</th>\n",
       "      <td>did:plc:znz5mi2kkmbpwqpgqsgtqhjf</td>\n",
       "      <td>at://did:plc:vdsijt3kvfafo7ng4i7r55ll/app.bsky...</td>\n",
       "      <td>2023-06-15 19:10:52.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312164</th>\n",
       "      <td>did:plc:znz5mi2kkmbpwqpgqsgtqhjf</td>\n",
       "      <td>at://did:plc:64mdicpo7sq4k5bx2z3m2jo6/app.bsky...</td>\n",
       "      <td>2023-06-15 22:23:33.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312165</th>\n",
       "      <td>did:plc:znz5mi2kkmbpwqpgqsgtqhjf</td>\n",
       "      <td>at://did:plc:nx3kofpg4oxmkonqr6su5lw4/app.bsky...</td>\n",
       "      <td>2023-06-15 22:28:06.388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312166 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            consumer_did  \\\n",
       "0       did:plc:bnvfxxa4jri24vhmupgdus7l   \n",
       "1       did:plc:bnvfxxa4jri24vhmupgdus7l   \n",
       "2       did:plc:bnvfxxa4jri24vhmupgdus7l   \n",
       "3       did:plc:bnvfxxa4jri24vhmupgdus7l   \n",
       "4       did:plc:bnvfxxa4jri24vhmupgdus7l   \n",
       "...                                  ...   \n",
       "312161  did:plc:znz5mi2kkmbpwqpgqsgtqhjf   \n",
       "312162  did:plc:znz5mi2kkmbpwqpgqsgtqhjf   \n",
       "312163  did:plc:znz5mi2kkmbpwqpgqsgtqhjf   \n",
       "312164  did:plc:znz5mi2kkmbpwqpgqsgtqhjf   \n",
       "312165  did:plc:znz5mi2kkmbpwqpgqsgtqhjf   \n",
       "\n",
       "                                                 post_uri  \\\n",
       "0       at://did:plc:623st67kkthivj4c6icvkqnq/app.bsky...   \n",
       "1       at://did:plc:vw2smontq2ruzmir67hj4igs/app.bsky...   \n",
       "2       at://did:plc:g3upacmdqfkflmiyvjdmv4wi/app.bsky...   \n",
       "3       at://did:plc:rfoctmk4guq56gensuermj7q/app.bsky...   \n",
       "4       at://did:plc:36ywnjbmhzu2umjtvuplrvp5/app.bsky...   \n",
       "...                                                   ...   \n",
       "312161  at://did:plc:4q5m3qghnw2uf6n2vucmrytc/app.bsky...   \n",
       "312162  at://did:plc:tdnmrckaby7w4ikdneqlbdai/app.bsky...   \n",
       "312163  at://did:plc:vdsijt3kvfafo7ng4i7r55ll/app.bsky...   \n",
       "312164  at://did:plc:64mdicpo7sq4k5bx2z3m2jo6/app.bsky...   \n",
       "312165  at://did:plc:nx3kofpg4oxmkonqr6su5lw4/app.bsky...   \n",
       "\n",
       "                     createdAt  \n",
       "0      2023-06-15 01:49:26.986  \n",
       "1      2023-06-15 07:09:49.695  \n",
       "2      2023-06-15 17:07:22.665  \n",
       "3      2023-06-15 17:08:16.896  \n",
       "4      2023-06-15 17:09:59.052  \n",
       "...                        ...  \n",
       "312161 2023-06-15 19:06:24.809  \n",
       "312162 2023-06-15 19:09:55.014  \n",
       "312163 2023-06-15 19:10:52.071  \n",
       "312164 2023-06-15 22:23:33.636  \n",
       "312165 2023-06-15 22:28:06.388  \n",
       "\n",
       "[312166 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likes_df = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        repo as consumer_did,  -- who did the liking\n",
    "        json_extract_string(record, '$.subject.uri') as post_uri,  -- which post was liked\n",
    "        createdAt -- when was it liked\n",
    "    FROM records \n",
    "    WHERE collection = 'app.bsky.feed.like'\n",
    "    AND createdAt >= '2023-06-15' AND createdAt < '2023-06-16'\n",
    "    -- Only include likes from consumers in training data\n",
    "    AND repo IN (SELECT DISTINCT consumer_did FROM train_edges_df)\n",
    "\"\"\").fetchdf()\n",
    "test_likes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95110"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likes_df['post_uri'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mappings...\n",
      "\n",
      "Stats:\n",
      "Number of unique posts: 95110\n",
      "Number of unique consumers: 132728\n",
      "Total number of likes: 312166\n"
     ]
    }
   ],
   "source": [
    "def save_mappings_pickle():\n",
    "    mappings = {\n",
    "        'post_to_idx': post_to_idx,\n",
    "        'consumer_to_idx': consumer_to_idx,\n",
    "        'post_likes': dict(post_likes)  # Convert defaultdict to dict for saving\n",
    "    }\n",
    "    with open('id_mappings.pkl', 'wb') as f:\n",
    "        pickle.dump(mappings, f)\n",
    "\n",
    "def load_mappings_pickle():\n",
    "    with open('id_mappings.pkl', 'rb') as f:\n",
    "        mappings = pickle.load(f)\n",
    "        post_to_idx = mappings['post_to_idx']\n",
    "        consumer_to_idx = mappings['consumer_to_idx']\n",
    "        post_likes = defaultdict(list, mappings['post_likes'])\n",
    "    return post_to_idx, consumer_to_idx, post_likes\n",
    "\n",
    "if os.path.exists('id_mappings.pkl'):\n",
    "    print(\"Loading mappings...\")\n",
    "    post_to_idx, consumer_to_idx, post_likes = load_mappings_pickle()\n",
    "else:\n",
    "    print(\"Creating mappings...\")\n",
    "    # Create post_to_idx mapping\n",
    "    post_to_idx = {uri: idx for idx, uri in enumerate(test_likes_df['post_uri'].unique())}\n",
    "\n",
    "    # Create reverse mappings for later reference\n",
    "    idx_to_post = {idx: uri for uri, idx in post_to_idx.items()}\n",
    "    idx_to_consumer = {idx: did for did, idx in consumer_to_idx.items()}\n",
    "\n",
    "    # Convert likes into a dictionary of lists where:\n",
    "    # key: post_idx\n",
    "    # value: list of tuples (consumer_idx, timestamp)\n",
    "    post_likes = defaultdict(list)\n",
    "    for _, row in tqdm(test_likes_df.iterrows()):\n",
    "        post_idx = post_to_idx[row['post_uri']]\n",
    "        consumer_idx = consumer_to_idx[row['consumer_did']]\n",
    "        timestamp = pd.Timestamp(row['createdAt']).timestamp()  # convert to Unix timestamp\n",
    "        post_likes[post_idx].append((consumer_idx, timestamp))\n",
    "\n",
    "    print(\"Saving mappings...\")\n",
    "    save_mappings_pickle()\n",
    "\n",
    "# Print stats\n",
    "print(\"\\nStats:\")\n",
    "print(f\"Number of unique posts: {len(post_to_idx)}\")\n",
    "print(f\"Number of unique consumers: {len(consumer_to_idx)}\")\n",
    "print(f\"Total number of likes: {sum(len(likes) for likes in post_likes.values())}\")\n",
    "\n",
    "# Now post_likes[post_idx] gives us a list of (consumer_idx, timestamp) tuples\n",
    "# We can use this for efficient post embedding computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply Producer Community Affinities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved post embeddings\n",
      "Post embedding shape: (95110, 64)\n",
      "Average L2 norm: 1.000\n",
      "Max L2 norm: 1.000\n",
      "\n",
      "Embedding value distribution:\n",
      "Mean: 0.002\n",
      "Std: 0.125\n",
      "Min: -0.873\n",
      "Max: 0.651\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Try to load saved embeddings first\n",
    "try:\n",
    "    post_embeddings = np.load('post_embeddings.npy')\n",
    "    print(\"Loaded saved post embeddings\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Computing post embeddings...\")\n",
    "    # Initialize array to store all post embeddings\n",
    "    embedding_dim = consumer_embeddings.shape[1]\n",
    "    post_embeddings = np.zeros((len(post_to_idx), embedding_dim))\n",
    "\n",
    "    # For each post, aggregate its likes\n",
    "    for post_idx, likes in tqdm(post_likes.items()):\n",
    "        # Get all consumer indices who liked this post\n",
    "        consumer_idxs = [like[0] for like in likes]\n",
    "        \n",
    "        # Get all relevant consumer embeddings at once\n",
    "        post_consumer_embeddings = consumer_embeddings[consumer_idxs]\n",
    "        \n",
    "        # Take mean of embeddings (already normalized)\n",
    "        mean_embedding = np.mean(post_consumer_embeddings, axis=0)\n",
    "        \n",
    "        # Normalize the mean embedding to unit length\n",
    "        mean_norm = np.linalg.norm(mean_embedding)\n",
    "        if mean_norm > 0:\n",
    "            post_embeddings[post_idx] = mean_embedding / mean_norm\n",
    "\n",
    "    # Save the computed embeddings\n",
    "    np.save('post_embeddings.npy', post_embeddings)\n",
    "    print(\"Saved post embeddings to post_embeddings.npy\")\n",
    "\n",
    "# Quick stats about the embeddings\n",
    "print(f\"Post embedding shape: {post_embeddings.shape}\")\n",
    "print(f\"Average L2 norm: {np.mean(np.linalg.norm(post_embeddings, axis=1)):.3f}\")\n",
    "print(f\"Max L2 norm: {np.max(np.linalg.norm(post_embeddings, axis=1)):.3f}\")\n",
    "\n",
    "# Let's also look at the distribution of values\n",
    "print(\"\\nEmbedding value distribution:\")\n",
    "print(f\"Mean: {np.mean(post_embeddings):.3f}\")\n",
    "print(f\"Std: {np.std(post_embeddings):.3f}\")\n",
    "print(f\"Min: {np.min(post_embeddings):.3f}\")\n",
    "print(f\"Max: {np.max(post_embeddings):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar posts to random post:\n",
      "\n",
      "Original post:\n",
      "Like count: 24\n",
      "Bsky link: https://bsky.app/profile/did:plc:o6ibgputv3kmdq6ebzd27ezx/post/3jy7cyftmcb25\n",
      "\n",
      "Similar posts:\n",
      "Similarity: 0.866\n",
      "Like count: 50\n",
      "Bsky link: https://bsky.app/profile/did:plc:7bdyw3t7ynnirri4m3dr3bjm/post/3jy6pgg3tfy25\n",
      "------------------------------\n",
      "Similarity: 0.810\n",
      "Like count: 41\n",
      "Bsky link: https://bsky.app/profile/did:plc:z5xzcmrkxxvvwzezdl3qeo53/post/3jyaa3kl7o52q\n",
      "------------------------------\n",
      "Similarity: 0.794\n",
      "Like count: 45\n",
      "Bsky link: https://bsky.app/profile/did:plc:qvzn322kmcvd7xtnips5xaun/post/3jy7wcyf7gt25\n",
      "------------------------------\n",
      "Similarity: 0.785\n",
      "Like count: 3\n",
      "Bsky link: https://bsky.app/profile/did:plc:dci5jlffbhbi4ui4inf2wk6i/post/3jy4lf6rsm42o\n",
      "------------------------------\n",
      "Similarity: 0.781\n",
      "Like count: 567\n",
      "Bsky link: https://bsky.app/profile/did:plc:o6ibgputv3kmdq6ebzd27ezx/post/3jyacmbmnr72y\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create idx_to_post mapping first\n",
    "idx_to_post = {idx: uri for uri, idx in post_to_idx.items()}\n",
    "\n",
    "# Helper function to convert post URI to Bluesky link\n",
    "def uri_to_bsky_link(uri):\n",
    "    # Example URI: at://did:plc:xyz/app.bsky.feed.post/tid\n",
    "    parts = uri.split('/')\n",
    "    did = parts[2]\n",
    "    tid = parts[-1]\n",
    "    return f\"https://bsky.app/profile/{did}/post/{tid}\"\n",
    "\n",
    "# Get a random post with more than 10 likes\n",
    "valid_post_idxs = [idx for idx in post_likes.keys() if len(post_likes[idx]) > 10]\n",
    "random_post_idx = np.random.choice(valid_post_idxs)\n",
    "post_uri = idx_to_post[random_post_idx]\n",
    "like_count = len(post_likes[random_post_idx])\n",
    "embedding_norm = np.linalg.norm(post_embeddings[random_post_idx])\n",
    "\n",
    "# Build FAISS index for fast similarity search\n",
    "import faiss\n",
    "\n",
    "# Normalize all embeddings for cosine similarity\n",
    "norms = np.linalg.norm(post_embeddings, axis=1, keepdims=True)\n",
    "normalized_embeddings = post_embeddings / norms\n",
    "\n",
    "# Build the index\n",
    "dimension = post_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product = cosine similarity for normalized vectors\n",
    "index.add(normalized_embeddings.astype('float32'))\n",
    "\n",
    "def find_similar_posts(query_idx, n=5):\n",
    "    query_embedding = normalized_embeddings[query_idx].reshape(1, -1).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, n+1)  # +1 because it will find the query itself\n",
    "    \n",
    "    # Remove the query itself (should be the first result)\n",
    "    distances = distances[0][1:]\n",
    "    indices = indices[0][1:]\n",
    "    \n",
    "    return list(zip(indices, distances))\n",
    "\n",
    "# Find similar posts for our random post\n",
    "print(\"\\nSimilar posts to random post:\")\n",
    "print(f\"\\nOriginal post:\")\n",
    "print(f\"Like count: {like_count}\")\n",
    "print(f\"Bsky link: {uri_to_bsky_link(post_uri)}\")\n",
    "print(\"\\nSimilar posts:\")\n",
    "similar_posts = find_similar_posts(random_post_idx)\n",
    "for similar_idx, similarity in similar_posts:\n",
    "    similar_uri = idx_to_post[similar_idx]\n",
    "    similar_likes = len(post_likes[similar_idx])\n",
    "    print(f\"Similarity: {similarity:.3f}\")\n",
    "    print(f\"Like count: {similar_likes}\")\n",
    "    print(f\"Bsky link: {uri_to_bsky_link(similar_uri)}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min created date: 2023-03-05 03:15:27.360000\n",
      "Max created date: 2023-06-16 08:14:46.365000\n",
      "Mean created date: 2023-06-14 06:22:41.856864\n",
      "Median created date: 2023-06-15 10:11:57.029000\n",
      "Std dev of created dates: 5 days 10:11:20.915355\n",
      "\n",
      "Quantiles:\n",
      "10th percentile: 2023-06-13 20:29:15.653400\n",
      "15th percentile: 2023-06-14 15:54:09.235900\n",
      "25th percentile: 2023-06-15 00:27:24.862500\n",
      "50th percentile: 2023-06-15 10:11:57.029000\n",
      "75th percentile: 2023-06-15 17:25:04.825750\n",
      "90th percentile: 2023-06-15 21:16:20.949100\n",
      "95th percentile: 2023-06-15 22:38:08.297200\n",
      "99th percentile: 2023-06-15 23:41:42.524600\n"
     ]
    }
   ],
   "source": [
    "# Get example posts from the test period\n",
    "example_posts_df = con.execute(\"\"\"\n",
    "    WITH posts AS (SELECT \n",
    "        substr(json_extract_string(record, '$.subject.uri'), \n",
    "               instr(json_extract_string(record, '$.subject.uri'), 'did:'),\n",
    "               instr(substr(json_extract_string(record, '$.subject.uri'), \n",
    "                          instr(json_extract_string(record, '$.subject.uri'), 'did:')), '/') - 1) as repo,\n",
    "        substr(json_extract_string(record, '$.subject.uri'),\n",
    "               instr(json_extract_string(record, '$.subject.uri'), 'post/') + 5) as rkey\n",
    "    FROM records \n",
    "    WHERE collection = 'app.bsky.feed.like'\n",
    "    AND createdAt >= '2023-06-15'\n",
    "    AND createdAt < '2023-06-16')\n",
    "    SELECT DISTINCT p.repo, p.rkey, r.createdAt\n",
    "    FROM posts p\n",
    "    JOIN records r ON r.repo = p.repo \n",
    "    WHERE r.collection = 'app.bsky.feed.post' AND r.createdAt >= '2023-03-01'\n",
    "    AND r.rkey = p.rkey\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"Min created date:\", example_posts_df['createdAt'].min())\n",
    "print(\"Max created date:\", example_posts_df['createdAt'].max())\n",
    "print(\"Mean created date:\", example_posts_df['createdAt'].mean())\n",
    "print(\"Median created date:\", example_posts_df['createdAt'].median())\n",
    "print(\"Std dev of created dates:\", example_posts_df['createdAt'].std())\n",
    "print(\"\\nQuantiles:\")\n",
    "print(\"10th percentile:\", example_posts_df['createdAt'].quantile(0.10))\n",
    "print(\"15th percentile:\", example_posts_df['createdAt'].quantile(0.15))\n",
    "print(\"25th percentile:\", example_posts_df['createdAt'].quantile(0.25))\n",
    "print(\"50th percentile:\", example_posts_df['createdAt'].quantile(0.50))\n",
    "print(\"75th percentile:\", example_posts_df['createdAt'].quantile(0.75))\n",
    "print(\"90th percentile:\", example_posts_df['createdAt'].quantile(0.90))\n",
    "print(\"95th percentile:\", example_posts_df['createdAt'].quantile(0.95))\n",
    "print(\"99th percentile:\", example_posts_df['createdAt'].quantile(0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test likes: 312166\n",
      "Number of unique consumers in test: 15833\n",
      "Number of unique posts in test: 95110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312166/312166 [00:12<00:00, 24037.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: redesign the evaluation to remove this\n",
    "\n",
    "print(f\"Number of test likes: {len(test_likes_df)}\")\n",
    "print(f\"Number of unique consumers in test: {test_likes_df['consumer_did'].nunique()}\")\n",
    "print(f\"Number of unique posts in test: {test_likes_df['post_uri'].nunique()}\")\n",
    "\n",
    "# For each consumer, get their liked posts\n",
    "test_consumer_likes = defaultdict(list)\n",
    "for _, row in tqdm(test_likes_df.iterrows(), total=len(test_likes_df)):\n",
    "    if row['consumer_did'] in consumer_to_idx:  # Only include known consumers\n",
    "        test_consumer_likes[row['consumer_did']].append(row['post_uri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized evaluation on 15833 test consumers.\n",
      "FAISS index built with 95110 posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch evaluating: 100%|██████████| 31/31 [00:38<00:00,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 20: Hit Rate = 0.684, Precision@20 = 0.155, Recall@20 = 0.347\n",
      "k = 50: Hit Rate = 0.718, Precision@50 = 0.086, Recall@50 = 0.395\n",
      "k = 100: Hit Rate = 0.750, Precision@100 = 0.053, Recall@100 = 0.439\n",
      "k = 500: Hit Rate = 0.845, Precision@500 = 0.017, Recall@500 = 0.591\n",
      "k = 1000: Hit Rate = 0.889, Precision@1000 = 0.011, Recall@1000 = 0.683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: redesign/refactor to speed up/fix memory issues. Plus get gpu evaluation to work.\n",
    "\n",
    "def vectorized_evaluation(test_likes_df, \n",
    "                          consumer_to_idx, \n",
    "                          post_to_idx, \n",
    "                          consumer_embeddings, \n",
    "                          post_embeddings, \n",
    "                          idx_to_post, \n",
    "                          k_list=[20, 50, 100, 500, 1000],\n",
    "                          batch_size=512):\n",
    "    \"\"\"\n",
    "    Vectorized evaluation in batches to reduce memory usage. This function:\n",
    "      - Filters test_likes_df to only include known consumers.\n",
    "      - Converts test likes into a grouped set of liked post indices per consumer.\n",
    "      - Processes consumers in batches:\n",
    "         - For each batch, builds a padded (dense) 'liked' matrix.\n",
    "         - Queries FAISS in batch for all test consumer embeddings.\n",
    "         - Uses broadcasting to compare recommended posts against liked posts.\n",
    "      - Computes hit rate, precision, and recall for each k value.\n",
    "      \n",
    "    Parameters:\n",
    "      test_likes_df      : DataFrame with columns 'consumer_did' and 'post_uri'\n",
    "      consumer_to_idx    : Mapping from consumer DID to consumer index\n",
    "      post_to_idx        : Mapping from post URI to post index\n",
    "      consumer_embeddings: NumPy array of consumer embeddings (shape: [n_consumers, D])\n",
    "      post_embeddings    : NumPy array of post embeddings (shape: [n_posts, D])\n",
    "      idx_to_post        : Reverse mapping from post index to post URI\n",
    "      k_list             : List of k values for which to compute metrics\n",
    "      batch_size         : Batch size used for processing consumers\n",
    "      \n",
    "    Returns:\n",
    "      final_metrics: A dictionary containing overall hit rate, precision, and recall per k.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Prepare the ground-truth liked posts ---\n",
    "    # Only consider test likes for known consumers.\n",
    "    test_likes_valid = test_likes_df[test_likes_df['consumer_did'].isin(consumer_to_idx)].copy()\n",
    "    # Map consumer_did and post_uri to persistent indices.\n",
    "    test_likes_valid['consumer_idx'] = test_likes_valid['consumer_did'].map(consumer_to_idx)\n",
    "    test_likes_valid['post_idx'] = test_likes_valid['post_uri'].map(post_to_idx)\n",
    "    \n",
    "    # Group test likes by consumer index to form a dict mapping consumer_idx -> list of post indices.\n",
    "    grouped = test_likes_valid.groupby('consumer_idx')['post_idx'].agg(list).reset_index()\n",
    "    \n",
    "    # Extract arrays.\n",
    "    consumer_indices = grouped['consumer_idx'].values.astype(np.int64)  # shape: (num_test_consumers,)\n",
    "    liked_lists = grouped['post_idx'].values  # each element is a list of post indices\n",
    "    \n",
    "    num_test_consumers = len(consumer_indices)\n",
    "    print(f\"Vectorized evaluation on {num_test_consumers} test consumers.\")\n",
    "    \n",
    "    # --- Set up FAISS ---\n",
    "    dimension = post_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # inner product is cosine similarity for normalized vectors\n",
    "    index.add(post_embeddings.astype('float32'))\n",
    "    print(\"FAISS index built with\", index.ntotal, \"posts.\")\n",
    "    \n",
    "    # --- Initialize accumulation for metrics ---\n",
    "    metrics_accum = {k: {\"hits\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"count\": 0} for k in k_list}\n",
    "    \n",
    "    max_k = max(k_list)\n",
    "    \n",
    "    # --- Process test consumers in batches ---\n",
    "    for start in tqdm(range(0, num_test_consumers, batch_size), desc=\"Batch evaluating\"):\n",
    "        end = min(start + batch_size, num_test_consumers)\n",
    "        batch_consumer_indices = consumer_indices[start:end]\n",
    "        # Get consumer embeddings for current batch.\n",
    "        batch_embeddings = consumer_embeddings[batch_consumer_indices].astype('float32')\n",
    "        \n",
    "        # Query FAISS for the top max_k recommendations for this batch.\n",
    "        distances, batch_recommended_indices = index.search(batch_embeddings, max_k)\n",
    "        # batch_recommended_indices shape: (current_batch_size, max_k)\n",
    "        \n",
    "        # Prepare a padded liked matrix for the consumers in this batch.\n",
    "        batch_liked_lists = [liked_lists[i] for i in range(start, end)]\n",
    "        # Determine maximum number of likes in this batch.\n",
    "        batch_max_likes = max(len(lst) for lst in batch_liked_lists)\n",
    "        # Create matrix (rows: consumers, columns: liked post indices), padded with -1.\n",
    "        liked_matrix = -np.ones((end - start, batch_max_likes), dtype=np.int32)\n",
    "        for i, lst in enumerate(batch_liked_lists):\n",
    "            liked_matrix[i, :len(lst)] = lst\n",
    "        # Count actual number of liked posts for each consumer.\n",
    "        liked_counts = (liked_matrix != -1).sum(axis=1)  # shape: (batch_size,)\n",
    "        \n",
    "        # --- Vectorized Intersection ---\n",
    "        # recommended_indices: shape (batch_size, max_k)\n",
    "        # Expand dimensions so that we can compare with the liked_matrix:\n",
    "        recommended_expanded = batch_recommended_indices[:, :, np.newaxis]  # (batch_size, max_k, 1)\n",
    "        liked_expanded = liked_matrix[:, np.newaxis, :]  # (batch_size, 1, batch_max_likes)\n",
    "        # Boolean matrix: True if recommended index is in liked list.\n",
    "        match_matrix = (recommended_expanded == liked_expanded)  # (batch_size, max_k, batch_max_likes)\n",
    "        is_match = np.any(match_matrix, axis=2)  # (batch_size, max_k)\n",
    "        \n",
    "        # --- Compute metrics for each k value ---\n",
    "        for k in k_list:\n",
    "            # Top k recommended matches.\n",
    "            topk_correct = is_match[:, :k].sum(axis=1)\n",
    "            precision = topk_correct / k\n",
    "            recall = topk_correct / liked_counts\n",
    "            hit = (topk_correct > 0).astype(np.int32)\n",
    "            \n",
    "            metrics_accum[k][\"hits\"] += np.sum(hit)\n",
    "            metrics_accum[k][\"precision\"] += np.sum(precision)\n",
    "            metrics_accum[k][\"recall\"] += np.sum(recall)\n",
    "            metrics_accum[k][\"count\"] += (end - start)\n",
    "    \n",
    "    # --- Average metrics over all evaluated consumers ---\n",
    "    final_metrics = {}\n",
    "    for k in k_list:\n",
    "        count = metrics_accum[k][\"count\"]\n",
    "        if count > 0:\n",
    "            avg_hit = metrics_accum[k][\"hits\"] / count\n",
    "            avg_precision = metrics_accum[k][\"precision\"] / count\n",
    "            avg_recall = metrics_accum[k][\"recall\"] / count\n",
    "            final_metrics[k] = {\n",
    "                \"hit_rate\": avg_hit,\n",
    "                \"precision\": avg_precision,\n",
    "                \"recall\": avg_recall\n",
    "            }\n",
    "            print(f\"k = {k}: Hit Rate = {avg_hit:.3f}, Precision@{k} = {avg_precision:.3f}, Recall@{k} = {avg_recall:.3f}\")\n",
    "        else:\n",
    "            final_metrics[k] = None\n",
    "            print(f\"k = {k}: No valid consumers were evaluated.\")\n",
    "            \n",
    "    return final_metrics\n",
    "\n",
    "# --- Usage Example ---\n",
    "# (Assumes test_likes_df, consumer_to_idx, post_to_idx,\n",
    "#  consumer_embeddings, post_embeddings, and idx_to_post are defined from previous steps.)\n",
    "metrics = vectorized_evaluation(test_likes_df, consumer_to_idx, post_to_idx,\n",
    "                                consumer_embeddings, post_embeddings, idx_to_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Working slow code\n",
    "# # TODO: make this 10x faster using GPU + vectorized operations\n",
    "\n",
    "# k_list = [20, 50, 100, 500, 1000]\n",
    "# max_k = max(k_list)\n",
    "\n",
    "# metrics = {k: {\"hits\": 0, \"precision\": 0.0, \"recall\": 0.0, \"count\": 0} for k in k_list}\n",
    "\n",
    "# # --- Build the FAISS index ---\n",
    "# dimension = post_embeddings.shape[1]\n",
    "# index = faiss.IndexFlatIP(dimension)  # Using inner product (cosine similarity when vectors are normalized)\n",
    "# index.add(post_embeddings.astype('float32'))\n",
    "# print(\"FAISS index built with\", index.ntotal, \"posts.\")\n",
    "\n",
    "# # --- Evaluation over all test consumers ---\n",
    "# # Assume test_consumer_likes is a dict mapping consumer id -> list of posts liked in the test period.\n",
    "# for consumer, liked_posts in tqdm(test_consumer_likes.items(), total=len(test_consumer_likes), desc=\"Evaluating consumers\"):\n",
    "#     # Ensure we only evaluate consumers with a corresponding training embedding and non-empty test likes.\n",
    "#     if consumer in consumer_to_idx and liked_posts:\n",
    "#         consumer_idx = consumer_to_idx[consumer]\n",
    "#         consumer_vec = consumer_embeddings[consumer_idx]\n",
    "        \n",
    "#         # Prepare the query vector for FAISS.\n",
    "#         query_vector = consumer_vec.astype('float32').reshape(1, -1)\n",
    "        \n",
    "#         # Query for the top max_k posts.\n",
    "#         distances, indices = index.search(query_vector, max_k)\n",
    "#         # Convert FAISS indices to post URIs using idx_to_post mapping.\n",
    "#         recommended_full = [idx_to_post[idx] for idx in indices[0]]\n",
    "        \n",
    "#         # For each k in our evaluation, slice out the top-k and update metrics.\n",
    "#         for k in k_list:\n",
    "#             recommended_posts = recommended_full[:k]\n",
    "#             # Compute the intersection of recommended posts with the posts actually liked by the consumer.\n",
    "#             hit_items = set(recommended_posts).intersection(set(liked_posts))\n",
    "#             hit = 1 if len(hit_items) > 0 else 0\n",
    "#             precision = len(hit_items) / k\n",
    "#             recall = len(hit_items) / len(liked_posts)\n",
    "            \n",
    "#             metrics[k][\"hits\"] += hit\n",
    "#             metrics[k][\"precision\"] += precision\n",
    "#             metrics[k][\"recall\"] += recall\n",
    "#             metrics[k][\"count\"] += 1\n",
    "\n",
    "# # Print out the average metrics for each k value.\n",
    "# print(\"\\nEvaluation Metrics:\")\n",
    "# for k in k_list:\n",
    "#     if metrics[k][\"count\"] > 0:\n",
    "#         avg_hit = metrics[k][\"hits\"] / metrics[k][\"count\"]\n",
    "#         avg_precision = metrics[k][\"precision\"] / metrics[k][\"count\"]\n",
    "#         avg_recall = metrics[k][\"recall\"] / metrics[k][\"count\"]\n",
    "#         print(f\"k = {k}: Hit Rate = {avg_hit:.3f}, Precision@{k} = {avg_precision:.3f}, Recall@{k} = {avg_recall:.3f}\")\n",
    "#     else:\n",
    "#         print(f\"k = {k}: No valid consumers were evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dims=4096\n",
    "# FAISS index built with 95110 posts.\n",
    "# Evaluating consumers: 100%|██████████| 15833/15833 [1:36:23<00:00,  2.74it/s]\n",
    "\n",
    "# Evaluation Metrics:\n",
    "# k = 20: Hit Rate = 0.926, Precision@20 = 0.287, Recall@20 = 0.721\n",
    "# k = 50: Hit Rate = 0.949, Precision@50 = 0.171, Recall@50 = 0.813\n",
    "# k = 100: Hit Rate = 0.963, Precision@100 = 0.110, Recall@100 = 0.872\n",
    "# k = 500: Hit Rate = 0.989, Precision@500 = 0.033, Recall@500 = 0.963\n",
    "# k = 1000: Hit Rate = 0.995, Precision@1000 = 0.018, Recall@1000 = 0.983"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
