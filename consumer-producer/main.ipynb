{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('records',)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from factorize import factorize\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import faiss\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Create/connect to DuckDB database\n",
    "con = duckdb.connect('../random_tests/scan_results.duckdb')\n",
    "con.execute(\"SHOW TABLES\").fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Producers (Users with >= 30 followers) before training cutoff date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>producer_did</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>did:plc:ztz3fmmgtlil47abbt7kl7gs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>did:plc:57u3t2eedsuuc5u7nvcpobh2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did:plc:kjixfa7wudorsmbyyfios3kp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>did:plc:gjqmj6z7sboffvvduvqd7oam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>did:plc:sw7clyzvb4xame6ffv2bui7c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37187</th>\n",
       "      <td>did:plc:gnbzbxppjkhcum6sirngqo6o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37188</th>\n",
       "      <td>did:plc:rha7jmdck6245vs5oalwz56l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37189</th>\n",
       "      <td>did:plc:ef6eor43k4mzs57l4adyntq7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37190</th>\n",
       "      <td>did:plc:gquqhkoyttfn4j2wkuipnjmw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37191</th>\n",
       "      <td>did:plc:qocxcqkbz747ytpuwkmsrrkj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37192 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           producer_did\n",
       "0      did:plc:ztz3fmmgtlil47abbt7kl7gs\n",
       "1      did:plc:57u3t2eedsuuc5u7nvcpobh2\n",
       "2      did:plc:kjixfa7wudorsmbyyfios3kp\n",
       "3      did:plc:gjqmj6z7sboffvvduvqd7oam\n",
       "4      did:plc:sw7clyzvb4xame6ffv2bui7c\n",
       "...                                 ...\n",
       "37187  did:plc:gnbzbxppjkhcum6sirngqo6o\n",
       "37188  did:plc:rha7jmdck6245vs5oalwz56l\n",
       "37189  did:plc:ef6eor43k4mzs57l4adyntq7\n",
       "37190  did:plc:gquqhkoyttfn4j2wkuipnjmw\n",
       "37191  did:plc:qocxcqkbz747ytpuwkmsrrkj\n",
       "\n",
       "[37192 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_producer_df = con.execute(\"\"\"\n",
    "WITH producers AS (\n",
    "    SELECT \n",
    "        json_extract_string(record, '$.subject') as producer_did\n",
    "    FROM records \n",
    "    WHERE collection = 'app.bsky.graph.follow'\n",
    "    AND createdAt < '2023-06-15'  -- before training cutoff date\n",
    "    GROUP BY json_extract_string(record, '$.subject')\n",
    "    HAVING COUNT(*) >= 30\n",
    ")\n",
    "SELECT producer_did\n",
    "FROM producers\n",
    "\"\"\").fetchdf()\n",
    "train_producer_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Training Edges (Consumer-Producer Follows Bipartite Graph)\n",
    "Excludes: \n",
    "- Follows after training cutoff date\n",
    "- Follows from producers with less than 30 followers\n",
    "- Posts before 2023-03-01 (Start of the network)\n",
    "- Likes before 2023-06-15 (Training period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumer_did</th>\n",
       "      <th>producer_did</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>did:plc:7hxhbhphfselzxjxhrxfykzr</td>\n",
       "      <td>did:plc:nvog7rczakwzh5ckxnjnwqdd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>did:plc:7hxhbhphfselzxjxhrxfykzr</td>\n",
       "      <td>did:plc:ohvstchboonnmbplvwkl33ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did:plc:7hxhbhphfselzxjxhrxfykzr</td>\n",
       "      <td>did:plc:cdgrfvzrwkcx6o6s4ek47k4o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>did:plc:7hxhbhphfselzxjxhrxfykzr</td>\n",
       "      <td>did:plc:sdxk3j4fv3nshpos7624mjjv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>did:plc:7hxhbhphfselzxjxhrxfykzr</td>\n",
       "      <td>did:plc:f5xkhushrnb4snbxuohamy4k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781719</th>\n",
       "      <td>did:plc:6af2nkhnmlgc2io3vxtt77jp</td>\n",
       "      <td>did:plc:ihcx4rndpxwg6ag6xwnuszcg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781720</th>\n",
       "      <td>did:plc:6af2nkhnmlgc2io3vxtt77jp</td>\n",
       "      <td>did:plc:xycm6fslm2hmg7c3h2ecfske</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781721</th>\n",
       "      <td>did:plc:6af2nkhnmlgc2io3vxtt77jp</td>\n",
       "      <td>did:plc:6hijiatj246jsxfqtnkyovy6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781722</th>\n",
       "      <td>did:plc:6af2nkhnmlgc2io3vxtt77jp</td>\n",
       "      <td>did:plc:w3wnj5nfcqxt26pmx3ajzsrb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5781723</th>\n",
       "      <td>did:plc:6af2nkhnmlgc2io3vxtt77jp</td>\n",
       "      <td>did:plc:fsrulnpihqtdky5vjff6ktf6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5781724 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             consumer_did                      producer_did\n",
       "0        did:plc:7hxhbhphfselzxjxhrxfykzr  did:plc:nvog7rczakwzh5ckxnjnwqdd\n",
       "1        did:plc:7hxhbhphfselzxjxhrxfykzr  did:plc:ohvstchboonnmbplvwkl33ko\n",
       "2        did:plc:7hxhbhphfselzxjxhrxfykzr  did:plc:cdgrfvzrwkcx6o6s4ek47k4o\n",
       "3        did:plc:7hxhbhphfselzxjxhrxfykzr  did:plc:sdxk3j4fv3nshpos7624mjjv\n",
       "4        did:plc:7hxhbhphfselzxjxhrxfykzr  did:plc:f5xkhushrnb4snbxuohamy4k\n",
       "...                                   ...                               ...\n",
       "5781719  did:plc:6af2nkhnmlgc2io3vxtt77jp  did:plc:ihcx4rndpxwg6ag6xwnuszcg\n",
       "5781720  did:plc:6af2nkhnmlgc2io3vxtt77jp  did:plc:xycm6fslm2hmg7c3h2ecfske\n",
       "5781721  did:plc:6af2nkhnmlgc2io3vxtt77jp  did:plc:6hijiatj246jsxfqtnkyovy6\n",
       "5781722  did:plc:6af2nkhnmlgc2io3vxtt77jp  did:plc:w3wnj5nfcqxt26pmx3ajzsrb\n",
       "5781723  did:plc:6af2nkhnmlgc2io3vxtt77jp  did:plc:fsrulnpihqtdky5vjff6ktf6\n",
       "\n",
       "[5781724 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the edges (consumer-producer relationships)\n",
    "train_edges_df = con.execute(\"\"\"\n",
    "SELECT \n",
    "    repo as consumer_did,\n",
    "    json_extract_string(record, '$.subject') as producer_did\n",
    "FROM records\n",
    "WHERE \n",
    "    collection = 'app.bsky.graph.follow'\n",
    "    AND json_extract_string(record, '$.subject') IN (SELECT producer_did FROM train_producer_df)\n",
    "\"\"\").fetchdf()\n",
    "train_edges_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Persistent Consumer/Producer ID Mappings. If you want to start over from scratch, delete the mappings files and rerun the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mappings unchanged, safe to use existing post embeddings.\n",
      "Matrix shape: (132728, 37192)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import hashlib\n",
    "\n",
    "def get_mapping_hash(mapping):\n",
    "    \"\"\"\n",
    "    Create a deterministic hash of a mapping dictionary.\n",
    "    \"\"\"\n",
    "    # Convert mapping to a sorted list of tuples to ensure consistent ordering\n",
    "    sorted_items = sorted(mapping.items())\n",
    "    # Convert to string and encode to bytes\n",
    "    mapping_str = json.dumps(sorted_items)\n",
    "    return hashlib.sha256(mapping_str.encode()).hexdigest()\n",
    "\n",
    "def load_mapping(mapping_file):\n",
    "    \"\"\"\n",
    "    Load a mapping from a JSON file. If the file doesn't exist, return an empty dict.\n",
    "    \"\"\"\n",
    "    if os.path.exists(mapping_file):\n",
    "        with open(mapping_file, \"r\") as f:\n",
    "            mapping = json.load(f)\n",
    "    else:\n",
    "        mapping = {}\n",
    "    return mapping\n",
    "\n",
    "def update_mapping(mapping, new_items):\n",
    "    \"\"\"\n",
    "    Update the mapping with new items. New items are appended by assigning \n",
    "    them an index equal to the current length of the mapping.\n",
    "    \"\"\"\n",
    "    for item in new_items:\n",
    "        if item not in mapping:\n",
    "            mapping[item] = len(mapping)\n",
    "    return mapping\n",
    "\n",
    "# File paths for persistent mappings\n",
    "consumer_mapping_file = 'consumer_mapping.json'\n",
    "producer_mapping_file = 'producer_mapping.json'\n",
    "hash_file = 'mappings_hash.json'\n",
    "\n",
    "# Load existing mappings (or create new ones if they don't exist)\n",
    "consumer_to_idx = load_mapping(consumer_mapping_file)\n",
    "producer_to_idx = load_mapping(producer_mapping_file)\n",
    "\n",
    "# Store original hashes\n",
    "original_hashes = {\n",
    "    'consumer': get_mapping_hash(consumer_to_idx),\n",
    "    'producer': get_mapping_hash(producer_to_idx)\n",
    "}\n",
    "\n",
    "# Get new DIDs from the current training data\n",
    "new_consumers = train_edges_df['consumer_did'].unique().tolist()\n",
    "new_producers = train_producer_df['producer_did'].unique().tolist()\n",
    "\n",
    "# Update the mappings with any new DIDs\n",
    "consumer_to_idx = update_mapping(consumer_to_idx, new_consumers)\n",
    "producer_to_idx = update_mapping(producer_to_idx, new_producers)\n",
    "\n",
    "# Get new hashes\n",
    "new_hashes = {\n",
    "    'consumer': get_mapping_hash(consumer_to_idx),\n",
    "    'producer': get_mapping_hash(producer_to_idx)\n",
    "}\n",
    "\n",
    "# Check if mappings changed\n",
    "mappings_changed = (original_hashes != new_hashes)\n",
    "\n",
    "if mappings_changed:\n",
    "    print(\"Warning: Mappings have changed! You should recompute post embeddings.\")\n",
    "    # Save the updated mappings to disk\n",
    "    with open(consumer_mapping_file, 'w') as f:\n",
    "        json.dump(consumer_to_idx, f)\n",
    "    with open(producer_mapping_file, 'w') as f:\n",
    "        json.dump(producer_to_idx, f)\n",
    "    # Save the new hashes\n",
    "    with open(hash_file, 'w') as f:\n",
    "        json.dump(new_hashes, f)\n",
    "else:\n",
    "    print(\"Mappings unchanged, safe to use existing post embeddings.\")\n",
    "\n",
    "# Create sparse matrix in COO format; each edge has weight 1\n",
    "rows = [consumer_to_idx[consumer] for consumer in train_edges_df['consumer_did']]\n",
    "cols = [producer_to_idx[producer] for producer in train_edges_df['producer_did']]\n",
    "data = np.ones(len(rows))\n",
    "\n",
    "# Build the sparse matrix (then convert to CSR format for efficient multiplication)\n",
    "matrix = sp.coo_matrix(\n",
    "    (data, (rows, cols)),\n",
    "    shape=(len(consumer_to_idx), len(producer_to_idx))\n",
    ")\n",
    "\n",
    "print(\"Matrix shape:\", matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it because for SVD I did L2 norm? Also look into deterministic and handling of new consumers/producers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer embeddings shape: (132728, 64)\n",
      "Average consumer embedding L2 norm: 1.000\n",
      "Average producer affinity: 0.491\n"
     ]
    }
   ],
   "source": [
    "# Usage example:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.manual_seed(42)  # IMPORTANT: temporary solution for deterministic results. Need this so that consumer_embeddings stays the same across runs.\n",
    "producer_communities, producer_community_affinities, consumer_embeddings, producer_embeddings, kmeans_cluster_centers = factorize(\n",
    "    matrix, \n",
    "    n_components=64,\n",
    "    n_clusters=100,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Print some stats\n",
    "print(f\"Consumer embeddings shape: {consumer_embeddings.shape}\")\n",
    "print(f\"Average consumer embedding L2 norm: {np.mean([np.linalg.norm(emb) for emb in consumer_embeddings]):.3f}\")\n",
    "print(f\"Average producer affinity: {producer_community_affinities.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.linalg.norm(emb) for emb in consumer_embeddings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average dimensions needed for 90% of magnitude: 40.9\n",
      "Median dimensions needed for 90% of magnitude: 41.0\n",
      "25th percentile dimensions: 39.0\n",
      "75th percentile dimensions: 43.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate how many dimensions are needed to reach 90% of total magnitude for each consumer\n",
    "consumer_magnitudes = np.sort(np.abs(consumer_embeddings), axis=1)[:, ::-1]  # Sort each row in descending order\n",
    "consumer_cumsum = np.cumsum(consumer_magnitudes, axis=1)\n",
    "consumer_totals = consumer_cumsum[:, -1:]  # Get final sums\n",
    "consumer_cumsum_norm = consumer_cumsum / consumer_totals  # Normalize to get cumulative percentages\n",
    "\n",
    "# Count dimensions needed for 90% per consumer\n",
    "dims_for_90 = np.sum(consumer_cumsum_norm < 0.9, axis=1) + 1\n",
    "print(f\"Average dimensions needed for 90% of magnitude: {dims_for_90.mean():.1f}\")\n",
    "print(f\"Median dimensions needed for 90% of magnitude: {np.median(dims_for_90):.1f}\")\n",
    "print(f\"25th percentile dimensions: {np.percentile(dims_for_90, 25):.1f}\")\n",
    "print(f\"75th percentile dimensions: {np.percentile(dims_for_90, 75):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leftover code; ignore producer_community_affinities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average affinity: 0.491\n",
      "Median affinity: 0.494\n",
      "25th percentile: 0.413\n",
      "75th percentile: 0.574\n",
      "Number of producers with affinity < 0.25: 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.42530587, 0.3979414 , 0.23629669, ..., 0.29527898, 0.23152331,\n",
       "       0.81280046])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Average affinity: {producer_community_affinities.mean():.3f}\")\n",
    "print(f\"Median affinity: {np.median(producer_community_affinities):.3f}\")\n",
    "print(f\"25th percentile: {np.percentile(producer_community_affinities, 25):.3f}\")\n",
    "print(f\"75th percentile: {np.percentile(producer_community_affinities, 75):.3f}\")\n",
    "print(f\"Number of producers with affinity < 0.25: {(producer_community_affinities < 0.25).sum()}\")\n",
    "producer_community_affinities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumer_did</th>\n",
       "      <th>post_uri</th>\n",
       "      <th>createdAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>did:plc:aoqgsg25dvpcubsqkpkr5on2</td>\n",
       "      <td>at://did:plc:v4ohdv3xxwoqbitlvaifelue/app.bsky...</td>\n",
       "      <td>2023-06-15 01:32:37.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>did:plc:aoqgsg25dvpcubsqkpkr5on2</td>\n",
       "      <td>at://did:plc:p7gxyfr5vii5ntpwo7f6dhe2/app.bsky...</td>\n",
       "      <td>2023-06-15 01:32:45.644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did:plc:aoqgsg25dvpcubsqkpkr5on2</td>\n",
       "      <td>at://did:plc:mqi7e5uxunzy4o75w2ddii3a/app.bsky...</td>\n",
       "      <td>2023-06-15 01:33:18.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>did:plc:aoqgsg25dvpcubsqkpkr5on2</td>\n",
       "      <td>at://did:plc:pdbljy6r5xannyn2ksdgqcj5/app.bsky...</td>\n",
       "      <td>2023-06-15 01:33:24.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>did:plc:aoqgsg25dvpcubsqkpkr5on2</td>\n",
       "      <td>at://did:plc:p7gxyfr5vii5ntpwo7f6dhe2/app.bsky...</td>\n",
       "      <td>2023-06-15 01:33:39.939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312161</th>\n",
       "      <td>did:plc:tbr4cunn6r4hox73ybzjfwt3</td>\n",
       "      <td>at://did:plc:odo2zkpujsgcxtz7ph24djkj/app.bsky...</td>\n",
       "      <td>2023-06-15 22:58:43.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312162</th>\n",
       "      <td>did:plc:3ds54cq7qb7u4cfbvoza5ur4</td>\n",
       "      <td>at://did:plc:biycslvww3shjifwhmb4gumh/app.bsky...</td>\n",
       "      <td>2023-06-15 03:13:03.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312163</th>\n",
       "      <td>did:plc:3ds54cq7qb7u4cfbvoza5ur4</td>\n",
       "      <td>at://did:plc:yxzcxzibalnlgslsi4dh4kqd/app.bsky...</td>\n",
       "      <td>2023-06-15 13:21:32.602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312164</th>\n",
       "      <td>did:plc:hcp53er6pefwijpdceo5x4bp</td>\n",
       "      <td>at://did:plc:apeaukvxm3yedgqw5zcf5pwc/app.bsky...</td>\n",
       "      <td>2023-06-15 03:53:35.783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312165</th>\n",
       "      <td>did:plc:hcp53er6pefwijpdceo5x4bp</td>\n",
       "      <td>at://did:plc:3x2yyg4s6c6n26caev222yv3/app.bsky...</td>\n",
       "      <td>2023-06-15 20:32:06.020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312166 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            consumer_did  \\\n",
       "0       did:plc:aoqgsg25dvpcubsqkpkr5on2   \n",
       "1       did:plc:aoqgsg25dvpcubsqkpkr5on2   \n",
       "2       did:plc:aoqgsg25dvpcubsqkpkr5on2   \n",
       "3       did:plc:aoqgsg25dvpcubsqkpkr5on2   \n",
       "4       did:plc:aoqgsg25dvpcubsqkpkr5on2   \n",
       "...                                  ...   \n",
       "312161  did:plc:tbr4cunn6r4hox73ybzjfwt3   \n",
       "312162  did:plc:3ds54cq7qb7u4cfbvoza5ur4   \n",
       "312163  did:plc:3ds54cq7qb7u4cfbvoza5ur4   \n",
       "312164  did:plc:hcp53er6pefwijpdceo5x4bp   \n",
       "312165  did:plc:hcp53er6pefwijpdceo5x4bp   \n",
       "\n",
       "                                                 post_uri  \\\n",
       "0       at://did:plc:v4ohdv3xxwoqbitlvaifelue/app.bsky...   \n",
       "1       at://did:plc:p7gxyfr5vii5ntpwo7f6dhe2/app.bsky...   \n",
       "2       at://did:plc:mqi7e5uxunzy4o75w2ddii3a/app.bsky...   \n",
       "3       at://did:plc:pdbljy6r5xannyn2ksdgqcj5/app.bsky...   \n",
       "4       at://did:plc:p7gxyfr5vii5ntpwo7f6dhe2/app.bsky...   \n",
       "...                                                   ...   \n",
       "312161  at://did:plc:odo2zkpujsgcxtz7ph24djkj/app.bsky...   \n",
       "312162  at://did:plc:biycslvww3shjifwhmb4gumh/app.bsky...   \n",
       "312163  at://did:plc:yxzcxzibalnlgslsi4dh4kqd/app.bsky...   \n",
       "312164  at://did:plc:apeaukvxm3yedgqw5zcf5pwc/app.bsky...   \n",
       "312165  at://did:plc:3x2yyg4s6c6n26caev222yv3/app.bsky...   \n",
       "\n",
       "                     createdAt  \n",
       "0      2023-06-15 01:32:37.648  \n",
       "1      2023-06-15 01:32:45.644  \n",
       "2      2023-06-15 01:33:18.992  \n",
       "3      2023-06-15 01:33:24.621  \n",
       "4      2023-06-15 01:33:39.939  \n",
       "...                        ...  \n",
       "312161 2023-06-15 22:58:43.084  \n",
       "312162 2023-06-15 03:13:03.891  \n",
       "312163 2023-06-15 13:21:32.602  \n",
       "312164 2023-06-15 03:53:35.783  \n",
       "312165 2023-06-15 20:32:06.020  \n",
       "\n",
       "[312166 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likes_df = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        repo as consumer_did,  -- who did the liking\n",
    "        json_extract_string(record, '$.subject.uri') as post_uri,  -- which post was liked\n",
    "        createdAt -- when was it liked\n",
    "    FROM records \n",
    "    WHERE collection = 'app.bsky.feed.like'\n",
    "    AND createdAt >= '2023-06-15' AND createdAt < '2023-06-16'\n",
    "    -- Only include likes from consumers in training data\n",
    "    AND repo IN (SELECT DISTINCT consumer_did FROM train_edges_df)\n",
    "\"\"\").fetchdf()\n",
    "test_likes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consumer/user embeddings come from the bipartite graph of consumers and producers. this doesn't change that much over the course of a day.\n",
    "\n",
    "consumer/producer bipartite graph is used to create the user embeddings\n",
    "\n",
    "\n",
    "post embeddings come from every user that liked the post before 06-15. Or it could come from 06-12 to 06-15. Or from 06-14 to 06-15.\n",
    "\n",
    "\n",
    "Right now in our code, post embeddings come from interactions from everything in 06-15.\n",
    "\n",
    "\n",
    "new post was created at 06-15 3am. Post embeddings also updated with likes after 3am.\n",
    "\n",
    "\n",
    "user1 embeddings -> represents basketball\n",
    "user2 embeddings -> represents music\n",
    "user3 embeddings -> represents basketball\n",
    "\n",
    "at t=2am\n",
    "post embedding = 0\n",
    "\n",
    "at t=3am  <--- we care about this time, cause we care about user1\n",
    "post embedding = user1\n",
    "\n",
    "at t=4am\n",
    "post embedding = user1 + user3\n",
    "\n",
    "at t=5am\n",
    "post embedding = user1 + user3 + user2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "post is supposed to have 10 likes at 3am. in 06-15 the whole 24 hours it has 30 likes. We remove the target user's like -> 29 likes. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Ideally, during recommendation, every single post's embedding should only come from the past interactions.\n",
    "\n",
    "We're in offline. \n",
    "\n",
    "We remove the current user we're trying to predict from the post embeddings. \n",
    "\n",
    "\n",
    "\n",
    "data structure: \n",
    "\n",
    "post: {(user1_id, timestamp1), (user2_id, timestamp2), ...}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95110"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likes_df['post_uri'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mappings...\n",
      "\n",
      "Stats:\n",
      "Number of unique posts: 95110\n",
      "Number of unique consumers: 132728\n",
      "Total number of likes: 312166\n"
     ]
    }
   ],
   "source": [
    "def save_mappings_pickle():\n",
    "    mappings = {\n",
    "        'post_to_idx': post_to_idx,\n",
    "        'consumer_to_idx': consumer_to_idx,\n",
    "        'post_likes': dict(post_likes)  # Convert defaultdict to dict for saving\n",
    "    }\n",
    "    with open('id_mappings.pkl', 'wb') as f:\n",
    "        pickle.dump(mappings, f)\n",
    "\n",
    "def load_mappings_pickle():\n",
    "    with open('id_mappings.pkl', 'rb') as f:\n",
    "        mappings = pickle.load(f)\n",
    "        post_to_idx = mappings['post_to_idx']\n",
    "        consumer_to_idx = mappings['consumer_to_idx']\n",
    "        post_likes = defaultdict(list, mappings['post_likes'])\n",
    "    return post_to_idx, consumer_to_idx, post_likes\n",
    "\n",
    "if os.path.exists('id_mappings.pkl'):\n",
    "    print(\"Loading mappings...\")\n",
    "    post_to_idx, consumer_to_idx, post_likes = load_mappings_pickle()\n",
    "else:\n",
    "    print(\"Creating mappings...\")\n",
    "    # Create post_to_idx mapping\n",
    "    post_to_idx = {uri: idx for idx, uri in enumerate(test_likes_df['post_uri'].unique())}\n",
    "\n",
    "    # Create reverse mappings for later reference\n",
    "    idx_to_post = {idx: uri for uri, idx in post_to_idx.items()}\n",
    "    idx_to_consumer = {idx: did for did, idx in consumer_to_idx.items()}\n",
    "\n",
    "    # Convert likes into a dictionary of lists where:\n",
    "    # key: post_idx\n",
    "    # value: list of tuples (consumer_idx, timestamp)\n",
    "    post_likes = defaultdict(list)\n",
    "    for _, row in tqdm(test_likes_df.iterrows()):\n",
    "        post_idx = post_to_idx[row['post_uri']]\n",
    "        consumer_idx = consumer_to_idx[row['consumer_did']]\n",
    "        timestamp = pd.Timestamp(row['createdAt']).timestamp()  # convert to Unix timestamp\n",
    "        post_likes[post_idx].append((consumer_idx, timestamp))\n",
    "\n",
    "    print(\"Saving mappings...\")\n",
    "    save_mappings_pickle()\n",
    "\n",
    "# Print stats\n",
    "print(\"\\nStats:\")\n",
    "print(f\"Number of unique posts: {len(post_to_idx)}\")\n",
    "print(f\"Number of unique consumers: {len(consumer_to_idx)}\")\n",
    "print(f\"Total number of likes: {sum(len(likes) for likes in post_likes.values())}\")\n",
    "\n",
    "# Now post_likes[post_idx] gives us a list of (consumer_idx, timestamp) tuples\n",
    "# We can use this for efficient post embedding computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded saved post embeddings\n",
      "Post embedding shape: (95110, 64)\n",
      "Average L2 norm: 1.000\n",
      "Max L2 norm: 1.000\n",
      "\n",
      "Embedding value distribution:\n",
      "Mean: 0.002\n",
      "Std: 0.125\n",
      "Min: -0.873\n",
      "Max: 0.651\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Try to load saved embeddings first\n",
    "try:\n",
    "    post_embeddings = np.load('post_embeddings.npy')\n",
    "    print(\"Loaded saved post embeddings\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Computing post embeddings...\")\n",
    "    # Initialize array to store all post embeddings\n",
    "    embedding_dim = consumer_embeddings.shape[1]\n",
    "    post_embeddings = np.zeros((len(post_to_idx), embedding_dim))\n",
    "\n",
    "    # For each post, aggregate its likes\n",
    "    for post_idx, likes in tqdm(post_likes.items()):\n",
    "        # Get all consumer indices who liked this post\n",
    "        consumer_idxs = [like[0] for like in likes]\n",
    "        \n",
    "        # Get all relevant consumer embeddings at once\n",
    "        post_consumer_embeddings = consumer_embeddings[consumer_idxs]\n",
    "        \n",
    "        # Take mean of embeddings (already normalized)\n",
    "        mean_embedding = np.mean(post_consumer_embeddings, axis=0)\n",
    "        \n",
    "        # Normalize the mean embedding to unit length\n",
    "        mean_norm = np.linalg.norm(mean_embedding)\n",
    "        if mean_norm > 0:\n",
    "            post_embeddings[post_idx] = mean_embedding / mean_norm\n",
    "\n",
    "    # Save the computed embeddings\n",
    "    np.save('post_embeddings.npy', post_embeddings)\n",
    "    print(\"Saved post embeddings to post_embeddings.npy\")\n",
    "\n",
    "# Quick stats about the embeddings\n",
    "print(f\"Post embedding shape: {post_embeddings.shape}\")\n",
    "print(f\"Average L2 norm: {np.mean(np.linalg.norm(post_embeddings, axis=1)):.3f}\")\n",
    "print(f\"Max L2 norm: {np.max(np.linalg.norm(post_embeddings, axis=1)):.3f}\")\n",
    "\n",
    "# Let's also look at the distribution of values\n",
    "print(\"\\nEmbedding value distribution:\")\n",
    "print(f\"Mean: {np.mean(post_embeddings):.3f}\")\n",
    "print(f\"Std: {np.std(post_embeddings):.3f}\")\n",
    "print(f\"Min: {np.min(post_embeddings):.3f}\")\n",
    "print(f\"Max: {np.max(post_embeddings):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Consumer:\n",
      "Consumer DID: did:plc:2sd4bexllgion2v5jenkmbfs\n",
      "Embedding L2 norm: 1.000\n",
      "Bsky profile link: https://bsky.app/profile/did:plc:2sd4bexllgion2v5jenkmbfs\n",
      "\n",
      "Similar users to the query consumer:\n",
      "Similarity: 0.732\n",
      "Consumer DID: did:plc:lylwnlnwfvae7tnlptydt5x7\n",
      "Bsky profile link: https://bsky.app/profile/did:plc:lylwnlnwfvae7tnlptydt5x7\n",
      "------------------------------\n",
      "Similarity: 0.729\n",
      "Consumer DID: did:plc:2jz37uks3semqepthxt7pmlp\n",
      "Bsky profile link: https://bsky.app/profile/did:plc:2jz37uks3semqepthxt7pmlp\n",
      "------------------------------\n",
      "Similarity: 0.724\n",
      "Consumer DID: did:plc:m2t2gk5gh3slgoscuec47p6u\n",
      "Bsky profile link: https://bsky.app/profile/did:plc:m2t2gk5gh3slgoscuec47p6u\n",
      "------------------------------\n",
      "Similarity: 0.717\n",
      "Consumer DID: did:plc:d65d7bgnnfziv7jogq3se33z\n",
      "Bsky profile link: https://bsky.app/profile/did:plc:d65d7bgnnfziv7jogq3se33z\n",
      "------------------------------\n",
      "Similarity: 0.712\n",
      "Consumer DID: did:plc:kyn27rdsbtvxxb6hywp5vjcf\n",
      "Bsky profile link: https://bsky.app/profile/did:plc:kyn27rdsbtvxxb6hywp5vjcf\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create idx_to_consumer mapping first (reverse mapping from consumer_did to index)\n",
    "idx_to_consumer = {idx: did for did, idx in consumer_to_idx.items()}\n",
    "\n",
    "# Helper function to convert a consumer DID to a Bluesky profile URL.\n",
    "def did_to_bsky_link(did):\n",
    "    return f\"https://bsky.app/profile/{did}\"\n",
    "\n",
    "# Select a random consumer from our mapping.\n",
    "random_consumer_idx = np.random.choice(list(idx_to_consumer.keys()))\n",
    "consumer_did = idx_to_consumer[random_consumer_idx]\n",
    "embedding_norm = np.linalg.norm(consumer_embeddings[random_consumer_idx])\n",
    "\n",
    "print(\"\\nQuery Consumer:\")\n",
    "print(f\"Consumer DID: {consumer_did}\")\n",
    "print(f\"Embedding L2 norm: {embedding_norm:.3f}\")\n",
    "print(f\"Bsky profile link: {did_to_bsky_link(consumer_did)}\")\n",
    "\n",
    "# Build FAISS index for consumer embeddings\n",
    "import faiss\n",
    "\n",
    "# Normalize all consumer embeddings for cosine similarity.\n",
    "norms_consumer = np.linalg.norm(consumer_embeddings, axis=1, keepdims=True)\n",
    "normalized_consumer_embeddings = consumer_embeddings / norms_consumer\n",
    "\n",
    "# Get dimensions from consumer embeddings.\n",
    "dimension = consumer_embeddings.shape[1]\n",
    "consumer_index = faiss.IndexFlatIP(dimension)  # Using inner product for cosine similarity (with normalized vectors).\n",
    "consumer_index.add(normalized_consumer_embeddings.astype('float32'))\n",
    "\n",
    "def find_similar_users(query_idx, n=5):\n",
    "    \"\"\"\n",
    "    Find similar users for the given query consumer index.\n",
    "    \n",
    "    Parameters:\n",
    "      query_idx (int): Index of the query consumer.\n",
    "      n (int): Number of similar users to return (excluding the query itself).\n",
    "    \n",
    "    Returns:\n",
    "      List of tuples (user_index, similarity_score)\n",
    "    \"\"\"\n",
    "    query_embedding = normalized_consumer_embeddings[query_idx].reshape(1, -1).astype('float32')\n",
    "    distances, indices = consumer_index.search(query_embedding, n + 1)  # +1 to account for the query itself.\n",
    "    # Remove the query itself (assumed to be the first result).\n",
    "    distances = distances[0][1:]\n",
    "    indices = indices[0][1:]\n",
    "    return list(zip(indices, distances))\n",
    "\n",
    "# Find similar users for our query consumer.\n",
    "print(\"\\nSimilar users to the query consumer:\")\n",
    "similar_users = find_similar_users(random_consumer_idx)\n",
    "for similar_idx, similarity in similar_users:\n",
    "    similar_did = idx_to_consumer[similar_idx]\n",
    "    print(f\"Similarity: {similarity:.3f}\")\n",
    "    print(f\"Consumer DID: {similar_did}\")\n",
    "    print(f\"Bsky profile link: {did_to_bsky_link(similar_did)}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similar posts to random post:\n",
      "\n",
      "Original post:\n",
      "Like count: 12\n",
      "Bsky link: https://bsky.app/profile/did:plc:axxuxcdopmhmru6vrmbiry3w/post/3jy75oefg7c2s\n",
      "\n",
      "Similar posts:\n",
      "Similarity: 0.952\n",
      "Like count: 4\n",
      "Bsky link: https://bsky.app/profile/did:plc:yvt2icpwwqbpkrz2dklxkuwt/post/3jy76p2y5dg2u\n",
      "------------------------------\n",
      "Similarity: 0.951\n",
      "Like count: 2\n",
      "Bsky link: https://bsky.app/profile/did:plc:ywbm3iywnhzep3ckt6efhoh7/post/3jy5zsvchmo2e\n",
      "------------------------------\n",
      "Similarity: 0.950\n",
      "Like count: 7\n",
      "Bsky link: https://bsky.app/profile/did:plc:axxuxcdopmhmru6vrmbiry3w/post/3jy7clfotl32s\n",
      "------------------------------\n",
      "Similarity: 0.946\n",
      "Like count: 2\n",
      "Bsky link: https://bsky.app/profile/did:plc:bsqgp7pjwiqqvsvym3drunk6/post/3jyafggvl5p2y\n",
      "------------------------------\n",
      "Similarity: 0.943\n",
      "Like count: 6\n",
      "Bsky link: https://bsky.app/profile/did:plc:axxuxcdopmhmru6vrmbiry3w/post/3jy7ljxicoa2y\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create idx_to_post mapping first\n",
    "idx_to_post = {idx: uri for uri, idx in post_to_idx.items()}\n",
    "\n",
    "# Helper function to convert post URI to Bluesky link\n",
    "def uri_to_bsky_link(uri):\n",
    "    # Example URI: at://did:plc:xyz/app.bsky.feed.post/tid\n",
    "    parts = uri.split('/')\n",
    "    did = parts[2]\n",
    "    tid = parts[-1]\n",
    "    return f\"https://bsky.app/profile/{did}/post/{tid}\"\n",
    "\n",
    "# Get a random post with more than 10 likes\n",
    "valid_post_idxs = [idx for idx in post_likes.keys() if len(post_likes[idx]) > 10]\n",
    "random_post_idx = np.random.choice(valid_post_idxs)\n",
    "post_uri = idx_to_post[random_post_idx]\n",
    "like_count = len(post_likes[random_post_idx])\n",
    "embedding_norm = np.linalg.norm(post_embeddings[random_post_idx])\n",
    "\n",
    "# Build FAISS index for fast similarity search\n",
    "import faiss\n",
    "\n",
    "# Normalize all embeddings for cosine similarity\n",
    "norms = np.linalg.norm(post_embeddings, axis=1, keepdims=True)\n",
    "normalized_embeddings = post_embeddings / norms\n",
    "\n",
    "# Build the index\n",
    "dimension = post_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Inner product = cosine similarity for normalized vectors\n",
    "index.add(normalized_embeddings.astype('float32'))\n",
    "\n",
    "def find_similar_posts(query_idx, n=5):\n",
    "    query_embedding = normalized_embeddings[query_idx].reshape(1, -1).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, n+1)  # +1 because it will find the query itself\n",
    "    \n",
    "    # Remove the query itself (should be the first result)\n",
    "    distances = distances[0][1:]\n",
    "    indices = indices[0][1:]\n",
    "    \n",
    "    return list(zip(indices, distances))\n",
    "\n",
    "# Find similar posts for our random post\n",
    "print(\"\\nSimilar posts to random post:\")\n",
    "print(f\"\\nOriginal post:\")\n",
    "print(f\"Like count: {like_count}\")\n",
    "print(f\"Bsky link: {uri_to_bsky_link(post_uri)}\")\n",
    "print(\"\\nSimilar posts:\")\n",
    "similar_posts = find_similar_posts(random_post_idx)\n",
    "for similar_idx, similarity in similar_posts:\n",
    "    similar_uri = idx_to_post[similar_idx]\n",
    "    similar_likes = len(post_likes[similar_idx])\n",
    "    print(f\"Similarity: {similarity:.3f}\")\n",
    "    print(f\"Like count: {similar_likes}\")\n",
    "    print(f\"Bsky link: {uri_to_bsky_link(similar_uri)}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min created date: 2023-03-05 03:15:27.360000\n",
      "Max created date: 2023-06-16 08:14:46.365000\n",
      "Mean created date: 2023-06-14 06:22:41.856864\n",
      "Median created date: 2023-06-15 10:11:57.029000\n",
      "Std dev of created dates: 5 days 10:11:20.915355\n",
      "\n",
      "Quantiles:\n",
      "10th percentile: 2023-06-13 20:29:15.653400\n",
      "15th percentile: 2023-06-14 15:54:09.235900\n",
      "25th percentile: 2023-06-15 00:27:24.862500\n",
      "50th percentile: 2023-06-15 10:11:57.029000\n",
      "75th percentile: 2023-06-15 17:25:04.825750\n",
      "90th percentile: 2023-06-15 21:16:20.949100\n",
      "95th percentile: 2023-06-15 22:38:08.297200\n",
      "99th percentile: 2023-06-15 23:41:42.524600\n"
     ]
    }
   ],
   "source": [
    "# Get example posts from the test period\n",
    "example_posts_df = con.execute(\"\"\"\n",
    "    WITH posts AS (SELECT \n",
    "        substr(json_extract_string(record, '$.subject.uri'), \n",
    "               instr(json_extract_string(record, '$.subject.uri'), 'did:'),\n",
    "               instr(substr(json_extract_string(record, '$.subject.uri'), \n",
    "                          instr(json_extract_string(record, '$.subject.uri'), 'did:')), '/') - 1) as repo,\n",
    "        substr(json_extract_string(record, '$.subject.uri'),\n",
    "               instr(json_extract_string(record, '$.subject.uri'), 'post/') + 5) as rkey\n",
    "    FROM records \n",
    "    WHERE collection = 'app.bsky.feed.like'\n",
    "    AND createdAt >= '2023-06-15'\n",
    "    AND createdAt < '2023-06-16')\n",
    "    SELECT DISTINCT p.repo, p.rkey, r.createdAt\n",
    "    FROM posts p\n",
    "    JOIN records r ON r.repo = p.repo \n",
    "    WHERE r.collection = 'app.bsky.feed.post' AND r.createdAt >= '2023-03-01'\n",
    "    AND r.rkey = p.rkey\n",
    "\"\"\").fetchdf()\n",
    "\n",
    "print(\"Min created date:\", example_posts_df['createdAt'].min())\n",
    "print(\"Max created date:\", example_posts_df['createdAt'].max())\n",
    "print(\"Mean created date:\", example_posts_df['createdAt'].mean())\n",
    "print(\"Median created date:\", example_posts_df['createdAt'].median())\n",
    "print(\"Std dev of created dates:\", example_posts_df['createdAt'].std())\n",
    "print(\"\\nQuantiles:\")\n",
    "print(\"10th percentile:\", example_posts_df['createdAt'].quantile(0.10))\n",
    "print(\"15th percentile:\", example_posts_df['createdAt'].quantile(0.15))\n",
    "print(\"25th percentile:\", example_posts_df['createdAt'].quantile(0.25))\n",
    "print(\"50th percentile:\", example_posts_df['createdAt'].quantile(0.50))\n",
    "print(\"75th percentile:\", example_posts_df['createdAt'].quantile(0.75))\n",
    "print(\"90th percentile:\", example_posts_df['createdAt'].quantile(0.90))\n",
    "print(\"95th percentile:\", example_posts_df['createdAt'].quantile(0.95))\n",
    "print(\"99th percentile:\", example_posts_df['createdAt'].quantile(0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_uri</th>\n",
       "      <th>post_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>at://did:plc:vdt7uhpicftcrwovx2nsf4cs/app.bsky...</td>\n",
       "      <td>2023-06-14 20:21:00.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>at://did:plc:vdt7uhpicftcrwovx2nsf4cs/app.bsky...</td>\n",
       "      <td>2023-06-14 21:30:41.579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>at://did:plc:vdt7uhpicftcrwovx2nsf4cs/app.bsky...</td>\n",
       "      <td>2023-06-15 11:58:47.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at://did:plc:vdt7uhpicftcrwovx2nsf4cs/app.bsky...</td>\n",
       "      <td>2023-06-15 19:15:14.635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>at://did:plc:vdt7uhpicftcrwovx2nsf4cs/app.bsky...</td>\n",
       "      <td>2023-06-15 21:34:59.106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71491</th>\n",
       "      <td>at://did:plc:3u6amjpr67qxxxfd2dvjfd5y/app.bsky...</td>\n",
       "      <td>2023-06-15 16:56:04.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71492</th>\n",
       "      <td>at://did:plc:ndnkcgglid6ylmhdqvi22n4z/app.bsky...</td>\n",
       "      <td>2023-06-14 15:37:08.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71493</th>\n",
       "      <td>at://did:plc:ndnkcgglid6ylmhdqvi22n4z/app.bsky...</td>\n",
       "      <td>2023-06-15 01:13:20.658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71494</th>\n",
       "      <td>at://did:plc:ndnkcgglid6ylmhdqvi22n4z/app.bsky...</td>\n",
       "      <td>2023-06-15 17:31:02.287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71495</th>\n",
       "      <td>at://did:plc:ndnkcgglid6ylmhdqvi22n4z/app.bsky...</td>\n",
       "      <td>2023-06-14 03:38:09.824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71496 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post_uri  \\\n",
       "0      at://did:plc:vdt7uhpicftcrwovx2nsf4cs/app.bsky...   \n",
       "1      at://did:plc:vdt7uhpicftcrwovx2nsf4cs/app.bsky...   \n",
       "2      at://did:plc:vdt7uhpicftcrwovx2nsf4cs/app.bsky...   \n",
       "3      at://did:plc:vdt7uhpicftcrwovx2nsf4cs/app.bsky...   \n",
       "4      at://did:plc:vdt7uhpicftcrwovx2nsf4cs/app.bsky...   \n",
       "...                                                  ...   \n",
       "71491  at://did:plc:3u6amjpr67qxxxfd2dvjfd5y/app.bsky...   \n",
       "71492  at://did:plc:ndnkcgglid6ylmhdqvi22n4z/app.bsky...   \n",
       "71493  at://did:plc:ndnkcgglid6ylmhdqvi22n4z/app.bsky...   \n",
       "71494  at://did:plc:ndnkcgglid6ylmhdqvi22n4z/app.bsky...   \n",
       "71495  at://did:plc:ndnkcgglid6ylmhdqvi22n4z/app.bsky...   \n",
       "\n",
       "               post_timestamp  \n",
       "0     2023-06-14 20:21:00.233  \n",
       "1     2023-06-14 21:30:41.579  \n",
       "2     2023-06-15 11:58:47.229  \n",
       "3     2023-06-15 19:15:14.635  \n",
       "4     2023-06-15 21:34:59.106  \n",
       "...                       ...  \n",
       "71491 2023-06-15 16:56:04.690  \n",
       "71492 2023-06-14 15:37:08.991  \n",
       "71493 2023-06-15 01:13:20.658  \n",
       "71494 2023-06-15 17:31:02.287  \n",
       "71495 2023-06-14 03:38:09.824  \n",
       "\n",
       "[71496 rows x 2 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get posts that were liked on June 15th and their original creation timestamps\n",
    "example_posts_df2 = con.execute(\"\"\"\n",
    "    WITH posts AS (SELECT \n",
    "        json_extract_string(record, '$.subject.uri') as post_uri\n",
    "    FROM records \n",
    "    WHERE collection = 'app.bsky.feed.like'\n",
    "    AND createdAt >= '2023-06-15'\n",
    "    AND createdAt < '2023-06-16')\n",
    "    SELECT DISTINCT p.post_uri, r.createdAt as post_timestamp\n",
    "    FROM posts p\n",
    "    JOIN records r ON r.repo = substr(p.post_uri,\n",
    "                                    instr(p.post_uri, 'did:'),\n",
    "                                    instr(substr(p.post_uri, instr(p.post_uri, 'did:')), '/') - 1)\n",
    "    WHERE r.collection = 'app.bsky.feed.post' \n",
    "    AND r.createdAt >= '2023-03-01'\n",
    "    AND r.rkey = substr(p.post_uri,\n",
    "                       instr(p.post_uri, 'post/') + 5)\n",
    "\"\"\").fetchdf()\n",
    "example_posts_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_uri</th>\n",
       "      <th>post_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>at://did:plc:cabm2libde26dxie4s5dtxsu/app.bsky...</td>\n",
       "      <td>2023-06-14 12:47:37.186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>at://did:plc:cabm2libde26dxie4s5dtxsu/app.bsky...</td>\n",
       "      <td>2023-06-15 06:35:07.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>at://did:plc:cabm2libde26dxie4s5dtxsu/app.bsky...</td>\n",
       "      <td>2023-06-15 15:43:18.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>at://did:plc:cabm2libde26dxie4s5dtxsu/app.bsky...</td>\n",
       "      <td>2023-06-15 16:46:47.747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>at://did:plc:rormbps7zdlq3bnub3vrqayh/app.bsky...</td>\n",
       "      <td>2023-06-15 00:34:13.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71275</th>\n",
       "      <td>at://did:plc:etmbmudpjdkykkirsjfa5fpw/app.bsky...</td>\n",
       "      <td>2023-06-14 18:40:55.699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71276</th>\n",
       "      <td>at://did:plc:hj6r66dul52rvakd6tdlux76/app.bsky...</td>\n",
       "      <td>2023-06-15 16:03:09.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71277</th>\n",
       "      <td>at://did:plc:ejdd67wq3xwtdfhcicwtdvay/app.bsky...</td>\n",
       "      <td>2023-06-15 13:51:54.309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71278</th>\n",
       "      <td>at://did:plc:jxnku2asndfq2wvovkmpklal/app.bsky...</td>\n",
       "      <td>2023-06-15 09:25:42.748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71279</th>\n",
       "      <td>at://did:plc:o42ydy2ihgfqp4ux2lmvu3qi/app.bsky...</td>\n",
       "      <td>2023-06-09 01:46:53.505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71280 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                post_uri  \\\n",
       "0      at://did:plc:cabm2libde26dxie4s5dtxsu/app.bsky...   \n",
       "1      at://did:plc:cabm2libde26dxie4s5dtxsu/app.bsky...   \n",
       "2      at://did:plc:cabm2libde26dxie4s5dtxsu/app.bsky...   \n",
       "3      at://did:plc:cabm2libde26dxie4s5dtxsu/app.bsky...   \n",
       "4      at://did:plc:rormbps7zdlq3bnub3vrqayh/app.bsky...   \n",
       "...                                                  ...   \n",
       "71275  at://did:plc:etmbmudpjdkykkirsjfa5fpw/app.bsky...   \n",
       "71276  at://did:plc:hj6r66dul52rvakd6tdlux76/app.bsky...   \n",
       "71277  at://did:plc:ejdd67wq3xwtdfhcicwtdvay/app.bsky...   \n",
       "71278  at://did:plc:jxnku2asndfq2wvovkmpklal/app.bsky...   \n",
       "71279  at://did:plc:o42ydy2ihgfqp4ux2lmvu3qi/app.bsky...   \n",
       "\n",
       "               post_timestamp  \n",
       "0     2023-06-14 12:47:37.186  \n",
       "1     2023-06-15 06:35:07.086  \n",
       "2     2023-06-15 15:43:18.032  \n",
       "3     2023-06-15 16:46:47.747  \n",
       "4     2023-06-15 00:34:13.875  \n",
       "...                       ...  \n",
       "71275 2023-06-14 18:40:55.699  \n",
       "71276 2023-06-15 16:03:09.462  \n",
       "71277 2023-06-15 13:51:54.309  \n",
       "71278 2023-06-15 09:25:42.748  \n",
       "71279 2023-06-09 01:46:53.505  \n",
       "\n",
       "[71280 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get posts that were liked and their original creation timestamps\n",
    "example_posts_df2 = con.execute(\"\"\"\n",
    "    WITH posts AS (SELECT DISTINCT post_uri\n",
    "    FROM test_likes_df)\n",
    "    SELECT DISTINCT p.post_uri, r.createdAt as post_timestamp\n",
    "    FROM posts p\n",
    "    JOIN records r ON r.repo = substr(p.post_uri,\n",
    "                                    instr(p.post_uri, 'did:'),\n",
    "                                    instr(substr(p.post_uri, instr(p.post_uri, 'did:')), '/') - 1)\n",
    "    WHERE r.collection = 'app.bsky.feed.post' \n",
    "    AND r.createdAt >= '2023-03-01'\n",
    "    AND r.rkey = substr(p.post_uri,\n",
    "                       instr(p.post_uri, 'post/') + 5)\n",
    "\"\"\").fetchdf()\n",
    "example_posts_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumer_did</th>\n",
       "      <th>post_uri</th>\n",
       "      <th>createdAt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>did:plc:aoqgsg25dvpcubsqkpkr5on2</td>\n",
       "      <td>at://did:plc:v4ohdv3xxwoqbitlvaifelue/app.bsky...</td>\n",
       "      <td>2023-06-15 01:32:37.648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>did:plc:aoqgsg25dvpcubsqkpkr5on2</td>\n",
       "      <td>at://did:plc:p7gxyfr5vii5ntpwo7f6dhe2/app.bsky...</td>\n",
       "      <td>2023-06-15 01:32:45.644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did:plc:aoqgsg25dvpcubsqkpkr5on2</td>\n",
       "      <td>at://did:plc:mqi7e5uxunzy4o75w2ddii3a/app.bsky...</td>\n",
       "      <td>2023-06-15 01:33:18.992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>did:plc:aoqgsg25dvpcubsqkpkr5on2</td>\n",
       "      <td>at://did:plc:pdbljy6r5xannyn2ksdgqcj5/app.bsky...</td>\n",
       "      <td>2023-06-15 01:33:24.621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>did:plc:aoqgsg25dvpcubsqkpkr5on2</td>\n",
       "      <td>at://did:plc:p7gxyfr5vii5ntpwo7f6dhe2/app.bsky...</td>\n",
       "      <td>2023-06-15 01:33:39.939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312161</th>\n",
       "      <td>did:plc:znz5mi2kkmbpwqpgqsgtqhjf</td>\n",
       "      <td>at://did:plc:4q5m3qghnw2uf6n2vucmrytc/app.bsky...</td>\n",
       "      <td>2023-06-15 19:06:24.809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312162</th>\n",
       "      <td>did:plc:znz5mi2kkmbpwqpgqsgtqhjf</td>\n",
       "      <td>at://did:plc:tdnmrckaby7w4ikdneqlbdai/app.bsky...</td>\n",
       "      <td>2023-06-15 19:09:55.014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312163</th>\n",
       "      <td>did:plc:znz5mi2kkmbpwqpgqsgtqhjf</td>\n",
       "      <td>at://did:plc:vdsijt3kvfafo7ng4i7r55ll/app.bsky...</td>\n",
       "      <td>2023-06-15 19:10:52.071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312164</th>\n",
       "      <td>did:plc:znz5mi2kkmbpwqpgqsgtqhjf</td>\n",
       "      <td>at://did:plc:64mdicpo7sq4k5bx2z3m2jo6/app.bsky...</td>\n",
       "      <td>2023-06-15 22:23:33.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312165</th>\n",
       "      <td>did:plc:znz5mi2kkmbpwqpgqsgtqhjf</td>\n",
       "      <td>at://did:plc:nx3kofpg4oxmkonqr6su5lw4/app.bsky...</td>\n",
       "      <td>2023-06-15 22:28:06.388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>312166 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            consumer_did  \\\n",
       "0       did:plc:aoqgsg25dvpcubsqkpkr5on2   \n",
       "1       did:plc:aoqgsg25dvpcubsqkpkr5on2   \n",
       "2       did:plc:aoqgsg25dvpcubsqkpkr5on2   \n",
       "3       did:plc:aoqgsg25dvpcubsqkpkr5on2   \n",
       "4       did:plc:aoqgsg25dvpcubsqkpkr5on2   \n",
       "...                                  ...   \n",
       "312161  did:plc:znz5mi2kkmbpwqpgqsgtqhjf   \n",
       "312162  did:plc:znz5mi2kkmbpwqpgqsgtqhjf   \n",
       "312163  did:plc:znz5mi2kkmbpwqpgqsgtqhjf   \n",
       "312164  did:plc:znz5mi2kkmbpwqpgqsgtqhjf   \n",
       "312165  did:plc:znz5mi2kkmbpwqpgqsgtqhjf   \n",
       "\n",
       "                                                 post_uri  \\\n",
       "0       at://did:plc:v4ohdv3xxwoqbitlvaifelue/app.bsky...   \n",
       "1       at://did:plc:p7gxyfr5vii5ntpwo7f6dhe2/app.bsky...   \n",
       "2       at://did:plc:mqi7e5uxunzy4o75w2ddii3a/app.bsky...   \n",
       "3       at://did:plc:pdbljy6r5xannyn2ksdgqcj5/app.bsky...   \n",
       "4       at://did:plc:p7gxyfr5vii5ntpwo7f6dhe2/app.bsky...   \n",
       "...                                                   ...   \n",
       "312161  at://did:plc:4q5m3qghnw2uf6n2vucmrytc/app.bsky...   \n",
       "312162  at://did:plc:tdnmrckaby7w4ikdneqlbdai/app.bsky...   \n",
       "312163  at://did:plc:vdsijt3kvfafo7ng4i7r55ll/app.bsky...   \n",
       "312164  at://did:plc:64mdicpo7sq4k5bx2z3m2jo6/app.bsky...   \n",
       "312165  at://did:plc:nx3kofpg4oxmkonqr6su5lw4/app.bsky...   \n",
       "\n",
       "                     createdAt  \n",
       "0      2023-06-15 01:32:37.648  \n",
       "1      2023-06-15 01:32:45.644  \n",
       "2      2023-06-15 01:33:18.992  \n",
       "3      2023-06-15 01:33:24.621  \n",
       "4      2023-06-15 01:33:39.939  \n",
       "...                        ...  \n",
       "312161 2023-06-15 19:06:24.809  \n",
       "312162 2023-06-15 19:09:55.014  \n",
       "312163 2023-06-15 19:10:52.071  \n",
       "312164 2023-06-15 22:23:33.636  \n",
       "312165 2023-06-15 22:28:06.388  \n",
       "\n",
       "[312166 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_likes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test likes: 312166\n",
      "Number of unique consumers in test: 15833\n",
      "Number of unique posts in test: 95110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 312166/312166 [00:14<00:00, 21635.50it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: redesign the evaluation to remove this\n",
    "\n",
    "print(f\"Number of test likes: {len(test_likes_df)}\")\n",
    "print(f\"Number of unique consumers in test: {test_likes_df['consumer_did'].nunique()}\")\n",
    "print(f\"Number of unique posts in test: {test_likes_df['post_uri'].nunique()}\")\n",
    "\n",
    "# For each consumer, get their liked posts\n",
    "test_consumer_likes = defaultdict(list)\n",
    "for _, row in tqdm(test_likes_df.iterrows(), total=len(test_likes_df)):\n",
    "    if row['consumer_did'] in consumer_to_idx:  # Only include known consumers\n",
    "        test_consumer_likes[row['consumer_did']].append(row['post_uri'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Streaming Evaluation (Doesn't use post embeddings we precomputed earlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Cache eligible posts incrementally: Since event timestamps are sorted, update the eligible set as time advances instead of filtering all posts every time.\n",
    "- Vectorize similarity computations: Stack eligible embeddings into a matrix and use vectorized dot products to compute cosine similarities rather than looping over dictionary items.\n",
    "These steps will reduce per-event overhead and leverage optimized numpy routines.\n",
    "- Add FAISS index to speed up similarity search.\n",
    "- 24 hour sliding window for recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Connected to DuckDB.\n",
      "[DEBUG] Loaded consumer mapping with 132728 entries.\n",
      "[DEBUG] Loaded post mapping with 95110 entries.\n",
      "[DEBUG] Built consumer_embeddings_dict with 132728 entries.\n",
      "[DEBUG] Reinitialized all post embeddings to zeros. Dictionary size: 95110\n",
      "[DEBUG] Retrieved post creation data with 71280 records.\n",
      "[DEBUG] Constructed post_creation_times for 95110 posts.\n",
      "[DEBUG] Valid post creation times count: 71280\n",
      "[DEBUG] Retrieved test likes data with 312166 records.\n",
      "[DEBUG] Total interactions for streaming evaluation: 312166\n",
      "[DEBUG] Total sorted interactions: 312166\n",
      "[DEBUG] First interaction: {'timestamp': 1686787200.261, 'user': 'did:plc:3gkot4qq3uzuvubg6hjnio4n', 'post': 'at://did:plc:xchz7ba6l4aswzfzpk5d3gq6/app.bsky.feed.post/3jy5yx6337d2r'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Streaming Eval:   0%|          | 0/312166 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Processing event 0: user=did:plc:3gkot4qq3uzuvubg6hjnio4n, liked_post=at://did:plc:xchz7ba6l4aswzfzpk5d3gq6/app.bsky.feed.post/3jy5yx6337d2r, timestamp=1686787200.261\n",
      "[DEBUG] Recent eligible posts count for event 0: 8801\n",
      "[DEBUG] Time range of recent eligible posts for event 0:\n",
      "        Oldest: 2023-06-13 17:00:22.676000\n",
      "        Newest: 2023-06-14 16:59:57.727000\n",
      "[DEBUG] Processing event 1: user=did:plc:rc6llk4jzhmmadkcrttlpwtv, liked_post=at://did:plc:myazd4xjh2sfdaq5wttlblgp/app.bsky.feed.post/3jy5wq6452v2e, timestamp=1686787200.852\n",
      "[DEBUG] Recent eligible posts count for event 1: 8801\n",
      "[DEBUG] Time range of recent eligible posts for event 1:\n",
      "        Oldest: 2023-06-13 17:00:22.676000\n",
      "        Newest: 2023-06-14 16:59:57.727000\n",
      "[DEBUG] Processing event 2: user=did:plc:6e7u34r4f6kp5irrslrj254f, liked_post=at://did:plc:eqs4oof55q3igzckhi7pbqa2/app.bsky.feed.post/3jy5v7y6y3a2d, timestamp=1686787201.289\n",
      "[DEBUG] Recent eligible posts count for event 2: 8801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Streaming Eval:   0%|          | 3/312166 [00:00<3:18:38, 26.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Time range of recent eligible posts for event 2:\n",
      "        Oldest: 2023-06-13 17:00:22.676000\n",
      "        Newest: 2023-06-14 16:59:57.727000\n",
      "[DEBUG] Processing event 3: user=did:plc:oaqkrcvgsklemfp2luhcpzyk, liked_post=at://did:plc:voly5c7kqgq6lezsil5rjykq/app.bsky.feed.post/3jy2mvv3z732e, timestamp=1686787201.53\n",
      "[DEBUG] Recent eligible posts count for event 3: 8802\n",
      "[DEBUG] Time range of recent eligible posts for event 3:\n",
      "        Oldest: 2023-06-13 17:00:22.676000\n",
      "        Newest: 2023-06-14 17:00:01.351000\n",
      "[DEBUG] Processing event 4: user=did:plc:6x4ufchubhotwzt2j4s4twbw, liked_post=at://did:plc:xyhhaslcpbujl3uctskzswh7/app.bsky.feed.post/3jy5dig7psy2z, timestamp=1686787201.749\n",
      "[DEBUG] Recent eligible posts count for event 4: 8802\n",
      "[DEBUG] Time range of recent eligible posts for event 4:\n",
      "        Oldest: 2023-06-13 17:00:22.676000\n",
      "        Newest: 2023-06-14 17:00:01.351000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Streaming Eval:  16%|█▌        | 50003/312166 [29:45<3:01:01, 24.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processed 50000 events.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Streaming Eval:  32%|███▏      | 100003/312166 [1:11:21<3:24:40, 17.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processed 100000 events.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Streaming Eval:  48%|████▊     | 150001/312166 [2:05:06<3:16:24, 13.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processed 150000 events.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Streaming Eval:  64%|██████▍   | 200001/312166 [3:12:27<2:42:42, 11.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processed 200000 events.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Streaming Eval:  80%|████████  | 250001/312166 [4:34:42<1:51:20,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processed 250000 events.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Streaming Eval:  96%|█████████▌| 300001/312166 [6:11:30<23:05,  8.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Processed 300000 events.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Streaming Eval: 100%|██████████| 312166/312166 [6:34:54<00:00, 13.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Real Data Streaming Evaluation (Past-Only) @ 20:\n",
      "  Hit Rate: 0.0085\n",
      "  Precision: 0.0004\n",
      "  Recall: 0.0085\n",
      "\n",
      "Real Data Streaming Evaluation (Past-Only) @ 50:\n",
      "  Hit Rate: 0.0207\n",
      "  Precision: 0.0004\n",
      "  Recall: 0.0207\n",
      "\n",
      "Real Data Streaming Evaluation (Past-Only) @ 100:\n",
      "  Hit Rate: 0.0388\n",
      "  Precision: 0.0004\n",
      "  Recall: 0.0388\n",
      "\n",
      "Real Data Streaming Evaluation (Past-Only) @ 500:\n",
      "  Hit Rate: 0.1388\n",
      "  Precision: 0.0003\n",
      "  Recall: 0.1388\n",
      "\n",
      "Real Data Streaming Evaluation (Past-Only) @ 1000:\n",
      "  Hit Rate: 0.2156\n",
      "  Precision: 0.0002\n",
      "  Recall: 0.2156\n",
      "\n",
      "Final Metrics Summary:\n",
      "k\tHit Rate\tPrecision\tRecall\n",
      "----------------------------------------\n",
      "20\t0.0085\t0.0004\t0.0085\n",
      "50\t0.0207\t0.0004\t0.0207\n",
      "100\t0.0388\t0.0004\t0.0388\n",
      "500\t0.1388\t0.0003\t0.1388\n",
      "1000\t0.2156\t0.0002\t0.2156\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "def update_embedding(current_embedding, consumer_embedding, count):\n",
    "    \"\"\"\n",
    "    Update the post embedding via incremental averaging.\n",
    "    \"\"\"\n",
    "    if count == 0:\n",
    "        new_embedding = consumer_embedding\n",
    "    else:\n",
    "        new_embedding = (current_embedding * count + consumer_embedding) / (count + 1)\n",
    "    norm = np.linalg.norm(new_embedding)\n",
    "    if norm > 0:\n",
    "        new_embedding = new_embedding / norm\n",
    "    # Debug print: new embedding norm (uncomment if needed)\n",
    "    # print(f\"[DEBUG] update_embedding: count={count}, new_emb_norm={np.linalg.norm(new_embedding):.4f}\")\n",
    "    return new_embedding\n",
    "\n",
    "def streaming_evaluation(interactions, consumer_embeddings, initial_post_embeddings, post_creation_times, k_list=[20, 50, 100, 500, 1000]):\n",
    "    \"\"\"\n",
    "    Streaming evaluation that only considers posts that have been created within the past 24 hours relative\n",
    "    to each event's timestamp, computing hit rates, precision, and recall for multiple k values.\n",
    "    \n",
    "    Parameters:\n",
    "      interactions           : List of dicts { 'timestamp': float, 'user': str, 'post': str }\n",
    "      consumer_embeddings    : dict mapping consumer id (DID) to embedding (numpy array)\n",
    "      initial_post_embeddings: dict mapping post URI to embedding (numpy array)\n",
    "      post_creation_times    : dict mapping post URI to creation timestamp (float)\n",
    "      k_list                 : List of top-k values for evaluation\n",
    "      \n",
    "    Returns:\n",
    "      metrics: Dictionary mapping each k to its metrics (hit rate, precision, recall)\n",
    "    \"\"\"\n",
    "    # Create a copy of post embeddings to allow in-place updates\n",
    "    post_embeddings = initial_post_embeddings.copy()\n",
    "    post_like_counts = {pid: 0 for pid in post_embeddings}\n",
    "    \n",
    "    # Initialize metrics records for each k value\n",
    "    hit_records = {k: [] for k in k_list}\n",
    "    precision_records = {k: [] for k in k_list}\n",
    "    recall_records = {k: [] for k in k_list}\n",
    "    \n",
    "    # Sort interactions by timestamp\n",
    "    interactions_sorted = sorted(interactions, key=lambda x: x['timestamp'])\n",
    "    print(f\"[DEBUG] Total sorted interactions: {len(interactions_sorted)}\")\n",
    "    if interactions_sorted:\n",
    "        print(f\"[DEBUG] First interaction: {interactions_sorted[0]}\")\n",
    "    \n",
    "    # Prepare a sorted list of post IDs based on their creation times for incremental caching\n",
    "    sorted_post_ids = sorted(post_embeddings.keys(), key=lambda pid: post_creation_times.get(pid, 0))\n",
    "    eligible_set = set()  # Cache: set of post IDs that have become eligible so far\n",
    "    eligible_idx = 0      # Pointer over the sorted_post_ids list\n",
    "    \n",
    "    # Define the time window (in seconds) for a 24-hour period\n",
    "    time_window = 24 * 60 * 60  # 24 hours in seconds\n",
    "    \n",
    "    for idx, event in enumerate(tqdm(interactions_sorted, desc=\"Streaming Eval\")):\n",
    "        current_time = event['timestamp']\n",
    "        user_id = event['user']\n",
    "        liked_post = event['post']\n",
    "        \n",
    "        # Retrieve consumer embedding (check exists)\n",
    "        if user_id not in consumer_embeddings:\n",
    "            print(f\"[WARNING] Consumer embedding not found for user: {user_id}. Skipping event.\")\n",
    "            for k in k_list:\n",
    "                hit_records[k].append(0)\n",
    "                precision_records[k].append(0)\n",
    "                recall_records[k].append(0)\n",
    "            continue\n",
    "        \n",
    "        consumer_emb = consumer_embeddings[user_id]\n",
    "        \n",
    "        # Incrementally add posts to the eligible set based on the current event's timestamp.\n",
    "        while eligible_idx < len(sorted_post_ids) and post_creation_times.get(sorted_post_ids[eligible_idx], 0) <= current_time:\n",
    "            eligible_set.add(sorted_post_ids[eligible_idx])\n",
    "            eligible_idx += 1\n",
    "        \n",
    "        # Filter the eligible set to include only posts created within the past 24 hours.\n",
    "        recent_eligible_posts = {\n",
    "            pid: post_embeddings[pid]\n",
    "            for pid in eligible_set\n",
    "            if post_creation_times.get(pid, 0) >= (current_time - time_window)\n",
    "        }\n",
    "        \n",
    "        # Print debug information every 50,000 events\n",
    "        if (idx + 1) % 50000 == 0:\n",
    "            print(f\"[DEBUG] Processing event {idx+1}: user={user_id}, liked_post={liked_post}, timestamp={current_time}\")\n",
    "            print(f\"[DEBUG] Recent eligible posts count for event {idx+1}: {len(recent_eligible_posts)}\")\n",
    "            if recent_eligible_posts:\n",
    "                oldest_post_time = min(post_creation_times.get(pid, 0) for pid in recent_eligible_posts)\n",
    "                newest_post_time = max(post_creation_times.get(pid, 0) for pid in recent_eligible_posts)\n",
    "                print(f\"[DEBUG] Time range of recent eligible posts for event {idx+1}:\")\n",
    "                print(f\"        Oldest: {datetime.fromtimestamp(oldest_post_time)}\")\n",
    "                print(f\"        Newest: {datetime.fromtimestamp(newest_post_time)}\")\n",
    "        \n",
    "        if not recent_eligible_posts:\n",
    "            for k in k_list:\n",
    "                hit_records[k].append(0)\n",
    "                precision_records[k].append(0)\n",
    "                recall_records[k].append(0)\n",
    "            continue\n",
    "        \n",
    "        # Compute cosine similarities (assumes embeddings are normalized)\n",
    "        similarities = {pid: np.dot(consumer_emb, emb) for pid, emb in recent_eligible_posts.items()}\n",
    "        # Get posts sorted by similarity in descending order\n",
    "        sorted_posts = sorted(similarities, key=similarities.get, reverse=True)\n",
    "        \n",
    "        # For each k in k_list, record metrics\n",
    "        for k in k_list:\n",
    "            recommended_posts = sorted_posts[:k]\n",
    "            # Since each event only has one relevant post (the liked post),\n",
    "            # relevant_and_recommended is 1 if the liked post is in the recommendations.\n",
    "            relevant_and_recommended = 1 if liked_post in recommended_posts else 0\n",
    "            \n",
    "            hit = relevant_and_recommended  # Hit: 1 if found, 0 if not.\n",
    "            precision = relevant_and_recommended / k\n",
    "            recall = relevant_and_recommended / 1  # Always 1 if found, 0 otherwise.\n",
    "            \n",
    "            hit_records[k].append(hit)\n",
    "            precision_records[k].append(precision)\n",
    "            recall_records[k].append(recall)\n",
    "        \n",
    "        # Update: update the liked post's embedding using the consumer embedding\n",
    "        if liked_post in post_embeddings:\n",
    "            current_emb = post_embeddings[liked_post]\n",
    "            count = post_like_counts[liked_post]\n",
    "            updated_emb = update_embedding(current_emb, consumer_emb, count)\n",
    "            post_embeddings[liked_post] = updated_emb\n",
    "            post_like_counts[liked_post] += 1\n",
    "        else:\n",
    "            print(f\"[WARNING] Liked post {liked_post} not found in post embeddings.\")\n",
    "        \n",
    "        # Optional: progress update is handled in the debug prints above\n",
    "        \n",
    "    # Compute overall metrics for each k\n",
    "    metrics = {}\n",
    "    for k in k_list:\n",
    "        metrics[k] = {\n",
    "            'hit_rate': np.mean(hit_records[k]) if hit_records[k] else 0.0,\n",
    "            'precision': np.mean(precision_records[k]) if precision_records[k] else 0.0,\n",
    "            'recall': np.mean(recall_records[k]) if recall_records[k] else 0.0\n",
    "        }\n",
    "        print(f\"\\nReal Data Streaming Evaluation (Past-Only) @ {k}:\")\n",
    "        print(f\"  Hit Rate: {metrics[k]['hit_rate']:.4f}\")\n",
    "        print(f\"  Precision: {metrics[k]['precision']:.4f}\")\n",
    "        print(f\"  Recall: {metrics[k]['recall']:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Connect to your DuckDB database.\n",
    "con = duckdb.connect('../random_tests/scan_results.duckdb')\n",
    "print(\"[DEBUG] Connected to DuckDB.\")\n",
    "\n",
    "# ------------------------------\n",
    "# Load persistent mappings & embeddings.\n",
    "# ------------------------------\n",
    "\n",
    "# Consumer mapping: consumer_did -> index\n",
    "with open('consumer_mapping.json', 'r') as f:\n",
    "    consumer_to_idx = json.load(f)\n",
    "print(f\"[DEBUG] Loaded consumer mapping with {len(consumer_to_idx)} entries.\")\n",
    "\n",
    "# Post mapping: post_to_idx is stored in our pickle file along with additional mappings.\n",
    "with open('id_mappings.pkl', 'rb') as f:\n",
    "    mappings = pickle.load(f)\n",
    "    post_to_idx = mappings['post_to_idx']\n",
    "print(f\"[DEBUG] Loaded post mapping with {len(post_to_idx)} entries.\")\n",
    "\n",
    "# Build a dictionary: consumer_did -> embedding vector.\n",
    "# Here, consumer_embeddings is assumed to be available from previous steps (e.g., from factorization)\n",
    "\n",
    "consumer_embeddings_dict = {did: consumer_embeddings[idx] for did, idx in consumer_to_idx.items()}\n",
    "print(f\"[DEBUG] Built consumer_embeddings_dict with {len(consumer_embeddings_dict)} entries.\")\n",
    "\n",
    "# Initialize all post embeddings to zeros.\n",
    "embedding_dim = post_embeddings.shape[1]  # Set to the dimensions of the post embeddings\n",
    "initial_post_embeddings = {uri: np.zeros(embedding_dim) for uri in post_to_idx.keys()}\n",
    "print(f\"[DEBUG] Reinitialized all post embeddings to zeros. Dictionary size: {len(initial_post_embeddings)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Load real post creation times.\n",
    "# ------------------------------\n",
    "\n",
    "# This query retrieves distinct post URIs from test_likes_df and then \n",
    "# joins them with the records table to obtain the creation timestamp.\n",
    "query = \"\"\"\n",
    "    WITH liked_posts AS (\n",
    "        SELECT DISTINCT post_uri\n",
    "        FROM test_likes_df\n",
    "    )\n",
    "    SELECT DISTINCT lp.post_uri, r.createdAt as post_timestamp\n",
    "    FROM liked_posts lp\n",
    "    JOIN records r \n",
    "      ON r.repo = substr(lp.post_uri, instr(lp.post_uri, 'did:'), \n",
    "                          instr(substr(lp.post_uri, instr(lp.post_uri, 'did:')), '/') - 1)\n",
    "         AND r.rkey = substr(lp.post_uri, instr(lp.post_uri, 'post/') + 5)\n",
    "    WHERE r.collection = 'app.bsky.feed.post'\n",
    "      AND r.createdAt >= '2023-03-01'\n",
    "\"\"\"\n",
    "post_creation_df = con.execute(query).fetchdf()\n",
    "print(f\"[DEBUG] Retrieved post creation data with {len(post_creation_df)} records.\")\n",
    "\n",
    "# Convert the DataFrame into a mapping: post_uri -> creation timestamp (as a float)\n",
    "post_creation_times = {\n",
    "    row['post_uri']: pd.Timestamp(row['post_timestamp']).timestamp() \n",
    "    for _, row in post_creation_df.iterrows()\n",
    "}\n",
    "\n",
    "# For any post in our mapping that is missing a creation time, assign a default (e.g. 0.0).\n",
    "for uri in post_to_idx.keys():\n",
    "    if uri not in post_creation_times:\n",
    "        post_creation_times[uri] = 0.0\n",
    "\n",
    "print(f\"[DEBUG] Constructed post_creation_times for {len(post_creation_times)} posts.\")\n",
    "\n",
    "valid_post_creation_times = {uri: ts for uri, ts in post_creation_times.items() if ts > 0.0}\n",
    "print(f\"[DEBUG] Valid post creation times count: {len(valid_post_creation_times)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Build the streaming interactions.\n",
    "# ------------------------------\n",
    "\n",
    "print(f\"[DEBUG] Retrieved test likes data with {len(test_likes_df)} records.\")\n",
    "\n",
    "interactions = []\n",
    "for _, row in test_likes_df.iterrows():\n",
    "    consumer_id = row['consumer_did']\n",
    "    post_id = row['post_uri']\n",
    "    # Only consider events where we have both consumer and post in our mappings.\n",
    "    if (consumer_id in consumer_to_idx) and (post_id in post_to_idx):\n",
    "        event_time = pd.Timestamp(row['createdAt']).timestamp()\n",
    "        interactions.append({\n",
    "            'timestamp': event_time,\n",
    "            'user': consumer_id,\n",
    "            'post': post_id\n",
    "        })\n",
    "print(f\"[DEBUG] Total interactions for streaming evaluation: {len(interactions)}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Run the streaming evaluation.\n",
    "# ------------------------------\n",
    "metrics = streaming_evaluation(\n",
    "    interactions, \n",
    "    consumer_embeddings_dict, \n",
    "    initial_post_embeddings, \n",
    "    valid_post_creation_times, \n",
    "    k_list=[20, 50, 100, 500, 1000]\n",
    ")\n",
    "\n",
    "print(\"\\nFinal Metrics Summary:\")\n",
    "print(\"k\\tHit Rate\\tPrecision\\tRecall\")\n",
    "print(\"-\" * 40)\n",
    "for k in sorted(metrics.keys()):\n",
    "    print(f\"{k}\\t{metrics[k]['hit_rate']:.4f}\\t{metrics[k]['precision']:.4f}\\t{metrics[k]['recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{20: 0.0077554890667145045,\n",
    " 50: 0.01946400312654165,\n",
    " 100: 0.03696110402798511,\n",
    " 500: 0.13867942056469956,\n",
    " 1000: 0.22101061614653741}\n",
    "\n",
    "Real Data Streaming Evaluation (Past-Only) @ 20:\n",
    "  Hit Rate: 0.0085\n",
    "  Precision: 0.0004\n",
    "  Recall: 0.0085\n",
    "\n",
    "Real Data Streaming Evaluation (Past-Only) @ 50:\n",
    "  Hit Rate: 0.0207\n",
    "  Precision: 0.0004\n",
    "  Recall: 0.0207\n",
    "\n",
    "Real Data Streaming Evaluation (Past-Only) @ 100:\n",
    "  Hit Rate: 0.0388\n",
    "  Precision: 0.0004\n",
    "  Recall: 0.0388\n",
    "\n",
    "Real Data Streaming Evaluation (Past-Only) @ 500:\n",
    "  Hit Rate: 0.1388\n",
    "  Precision: 0.0003\n",
    "  Recall: 0.1388\n",
    "\n",
    "Real Data Streaming Evaluation (Past-Only) @ 1000:\n",
    "  Hit Rate: 0.2156\n",
    "  Precision: 0.0002\n",
    "  Recall: 0.2156\n",
    "...\n",
    "50\t0.0207\t0.0004\t0.0207\n",
    "100\t0.0388\t0.0004\t0.0388\n",
    "500\t0.1388\t0.0003\t0.1388\n",
    "1000\t0.2156\t0.0002\t0.2156"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts with creation time = 0: 95110\n",
      "Total number of posts: 95110\n",
      "Percentage with creation time = 0: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Count how many posts have a creation time of 0\n",
    "zero_times = sum(1 for time in post_creation_times.values() if time == 0.0)\n",
    "print(f\"Number of posts with creation time = 0: {zero_times}\")\n",
    "print(f\"Total number of posts: {len(post_creation_times)}\")\n",
    "print(f\"Percentage with creation time = 0: {zero_times/len(post_creation_times)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized evaluation on 15833 test consumers.\n",
      "FAISS index built with 95110 posts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch evaluating:   6%|▋         | 2/31 [00:01<00:17,  1.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9829/1326523015.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# --- Usage Example ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;31m# (Assumes test_likes_df, consumer_to_idx, post_to_idx,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;31m#  consumer_embeddings, post_embeddings, and idx_to_post are defined from previous steps.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m metrics = vectorized_evaluation(test_likes_df, consumer_to_idx, post_to_idx,\n\u001b[0m\u001b[1;32m    131\u001b[0m                                 consumer_embeddings, post_embeddings, idx_to_post)\n",
      "\u001b[0;32m/tmp/ipykernel_9829/1326523015.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(test_likes_df, consumer_to_idx, post_to_idx, consumer_embeddings, post_embeddings, idx_to_post, k_list, batch_size)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Get consumer embeddings for current batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mbatch_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconsumer_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_consumer_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Query FAISS for the top max_k recommendations for this batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_recommended_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0;31m# batch_recommended_indices shape: (current_batch_size, max_k)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Prepare a padded liked matrix for the consumers in this batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Bluesky-GraphRec/.venv/lib/python3.10/site-packages/faiss/__init__.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x, k, D, I)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswig_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/Bluesky-GraphRec/.venv/lib/python3.10/site-packages/faiss/swigfaiss.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, n, x, k, distances, labels)\u001b[0m\n\u001b[1;32m   2145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexFlat_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: redesign/refactor to speed up/fix memory issues. Plus get gpu evaluation to work.\n",
    "\n",
    "def vectorized_evaluation(test_likes_df, \n",
    "                          consumer_to_idx, \n",
    "                          post_to_idx, \n",
    "                          consumer_embeddings, \n",
    "                          post_embeddings, \n",
    "                          idx_to_post, \n",
    "                          k_list=[20, 50, 100, 500, 1000],\n",
    "                          batch_size=512):\n",
    "    \"\"\"\n",
    "    Vectorized evaluation in batches to reduce memory usage. This function:\n",
    "      - Filters test_likes_df to only include known consumers.\n",
    "      - Converts test likes into a grouped set of liked post indices per consumer.\n",
    "      - Processes consumers in batches:\n",
    "         - For each batch, builds a padded (dense) 'liked' matrix.\n",
    "         - Queries FAISS in batch for all test consumer embeddings.\n",
    "         - Uses broadcasting to compare recommended posts against liked posts.\n",
    "      - Computes hit rate, precision, and recall for each k value.\n",
    "      \n",
    "    Parameters:\n",
    "      test_likes_df      : DataFrame with columns 'consumer_did' and 'post_uri'\n",
    "      consumer_to_idx    : Mapping from consumer DID to consumer index\n",
    "      post_to_idx        : Mapping from post URI to post index\n",
    "      consumer_embeddings: NumPy array of consumer embeddings (shape: [n_consumers, D])\n",
    "      post_embeddings    : NumPy array of post embeddings (shape: [n_posts, D])\n",
    "      idx_to_post        : Reverse mapping from post index to post URI\n",
    "      k_list             : List of k values for which to compute metrics\n",
    "      batch_size         : Batch size used for processing consumers\n",
    "      \n",
    "    Returns:\n",
    "      final_metrics: A dictionary containing overall hit rate, precision, and recall per k.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Prepare the ground-truth liked posts ---\n",
    "    # Only consider test likes for known consumers.\n",
    "    test_likes_valid = test_likes_df[test_likes_df['consumer_did'].isin(consumer_to_idx)].copy()\n",
    "    # Map consumer_did and post_uri to persistent indices.\n",
    "    test_likes_valid['consumer_idx'] = test_likes_valid['consumer_did'].map(consumer_to_idx)\n",
    "    test_likes_valid['post_idx'] = test_likes_valid['post_uri'].map(post_to_idx)\n",
    "    \n",
    "    # Group test likes by consumer index to form a dict mapping consumer_idx -> list of post indices.\n",
    "    grouped = test_likes_valid.groupby('consumer_idx')['post_idx'].agg(list).reset_index()\n",
    "    \n",
    "    # Extract arrays.\n",
    "    consumer_indices = grouped['consumer_idx'].values.astype(np.int64)  # shape: (num_test_consumers,)\n",
    "    liked_lists = grouped['post_idx'].values  # each element is a list of post indices\n",
    "    \n",
    "    num_test_consumers = len(consumer_indices)\n",
    "    print(f\"Vectorized evaluation on {num_test_consumers} test consumers.\")\n",
    "    \n",
    "    # --- Set up FAISS ---\n",
    "    dimension = post_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)  # inner product is cosine similarity for normalized vectors\n",
    "    index.add(post_embeddings.astype('float32'))\n",
    "    print(\"FAISS index built with\", index.ntotal, \"posts.\")\n",
    "    \n",
    "    # --- Initialize accumulation for metrics ---\n",
    "    metrics_accum = {k: {\"hits\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"count\": 0} for k in k_list}\n",
    "    \n",
    "    max_k = max(k_list)\n",
    "    \n",
    "    # --- Process test consumers in batches ---\n",
    "    for start in tqdm(range(0, num_test_consumers, batch_size), desc=\"Batch evaluating\"):\n",
    "        end = min(start + batch_size, num_test_consumers)\n",
    "        batch_consumer_indices = consumer_indices[start:end]\n",
    "        # Get consumer embeddings for current batch.\n",
    "        batch_embeddings = consumer_embeddings[batch_consumer_indices].astype('float32')\n",
    "        \n",
    "        # Query FAISS for the top max_k recommendations for this batch.\n",
    "        distances, batch_recommended_indices = index.search(batch_embeddings, max_k)\n",
    "        # batch_recommended_indices shape: (current_batch_size, max_k)\n",
    "        \n",
    "        # Prepare a padded liked matrix for the consumers in this batch.\n",
    "        batch_liked_lists = [liked_lists[i] for i in range(start, end)]\n",
    "        # Determine maximum number of likes in this batch.\n",
    "        batch_max_likes = max(len(lst) for lst in batch_liked_lists)\n",
    "        # Create matrix (rows: consumers, columns: liked post indices), padded with -1.\n",
    "        liked_matrix = -np.ones((end - start, batch_max_likes), dtype=np.int32)\n",
    "        for i, lst in enumerate(batch_liked_lists):\n",
    "            liked_matrix[i, :len(lst)] = lst\n",
    "        # Count actual number of liked posts for each consumer.\n",
    "        liked_counts = (liked_matrix != -1).sum(axis=1)  # shape: (batch_size,)\n",
    "        \n",
    "        # --- Vectorized Intersection ---\n",
    "        # recommended_indices: shape (batch_size, max_k)\n",
    "        # Expand dimensions so that we can compare with the liked_matrix:\n",
    "        recommended_expanded = batch_recommended_indices[:, :, np.newaxis]  # (batch_size, max_k, 1)\n",
    "        liked_expanded = liked_matrix[:, np.newaxis, :]  # (batch_size, 1, batch_max_likes)\n",
    "        # Boolean matrix: True if recommended index is in liked list.\n",
    "        match_matrix = (recommended_expanded == liked_expanded)  # (batch_size, max_k, batch_max_likes)\n",
    "        is_match = np.any(match_matrix, axis=2)  # (batch_size, max_k)\n",
    "        \n",
    "        # --- Compute metrics for each k value ---\n",
    "        for k in k_list:\n",
    "            # Top k recommended matches.\n",
    "            topk_correct = is_match[:, :k].sum(axis=1)\n",
    "            precision = topk_correct / k\n",
    "            recall = topk_correct / liked_counts\n",
    "            hit = (topk_correct > 0).astype(np.int32)\n",
    "            \n",
    "            metrics_accum[k][\"hits\"] += np.sum(hit)\n",
    "            metrics_accum[k][\"precision\"] += np.sum(precision)\n",
    "            metrics_accum[k][\"recall\"] += np.sum(recall)\n",
    "            metrics_accum[k][\"count\"] += (end - start)\n",
    "    \n",
    "    # --- Average metrics over all evaluated consumers ---\n",
    "    final_metrics = {}\n",
    "    for k in k_list:\n",
    "        count = metrics_accum[k][\"count\"]\n",
    "        if count > 0:\n",
    "            avg_hit = metrics_accum[k][\"hits\"] / count\n",
    "            avg_precision = metrics_accum[k][\"precision\"] / count\n",
    "            avg_recall = metrics_accum[k][\"recall\"] / count\n",
    "            final_metrics[k] = {\n",
    "                \"hit_rate\": avg_hit,\n",
    "                \"precision\": avg_precision,\n",
    "                \"recall\": avg_recall\n",
    "            }\n",
    "            print(f\"k = {k}: Hit Rate = {avg_hit:.3f}, Precision@{k} = {avg_precision:.3f}, Recall@{k} = {avg_recall:.3f}\")\n",
    "        else:\n",
    "            final_metrics[k] = None\n",
    "            print(f\"k = {k}: No valid consumers were evaluated.\")\n",
    "            \n",
    "    return final_metrics\n",
    "\n",
    "# --- Usage Example ---\n",
    "# (Assumes test_likes_df, consumer_to_idx, post_to_idx,\n",
    "#  consumer_embeddings, post_embeddings, and idx_to_post are defined from previous steps.)\n",
    "metrics = vectorized_evaluation(test_likes_df, consumer_to_idx, post_to_idx,\n",
    "                                consumer_embeddings, post_embeddings, idx_to_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "matrix factorization -> user embeddings\n",
    "\n",
    "user embedding 1 \n",
    "user embedding 2      \n",
    "user embedding 3   ----->  post embedding\n",
    "user embedding 4\n",
    "user embedding 5\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working slow code\n",
    "# TODO: make this 10x faster using GPU + vectorized operations\n",
    "\n",
    "k_list = [20, 50, 100, 500, 1000]\n",
    "max_k = max(k_list)\n",
    "\n",
    "metrics = {k: {\"hits\": 0, \"precision\": 0.0, \"recall\": 0.0, \"count\": 0} for k in k_list}\n",
    "\n",
    "# --- Build the FAISS index ---\n",
    "dimension = post_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # Using inner product (cosine similarity when vectors are normalized)\n",
    "index.add(post_embeddings.astype('float32'))\n",
    "print(\"FAISS index built with\", index.ntotal, \"posts.\")\n",
    "\n",
    "# --- Evaluation over all test consumers ---\n",
    "# Assume test_consumer_likes is a dict mapping consumer id -> list of posts liked in the test period.\n",
    "for consumer, liked_posts in tqdm(test_consumer_likes.items(), total=len(test_consumer_likes), desc=\"Evaluating consumers\"):\n",
    "    # Ensure we only evaluate consumers with a corresponding training embedding and non-empty test likes.\n",
    "    if consumer in consumer_to_idx and liked_posts:\n",
    "        consumer_idx = consumer_to_idx[consumer]\n",
    "        consumer_vec = consumer_embeddings[consumer_idx]\n",
    "        \n",
    "        # Prepare the query vector for FAISS.\n",
    "        query_vector = consumer_vec.astype('float32').reshape(1, -1)\n",
    "        \n",
    "        # Query for the top max_k posts.\n",
    "        distances, indices = index.search(query_vector, max_k)\n",
    "        # Convert FAISS indices to post URIs using idx_to_post mapping.\n",
    "        recommended_full = [idx_to_post[idx] for idx in indices[0]]\n",
    "        \n",
    "        # For each k in our evaluation, slice out the top-k and update metrics.\n",
    "        for k in k_list:\n",
    "            recommended_posts = recommended_full[:k]\n",
    "            # Compute the intersection of recommended posts with the posts actually liked by the consumer.\n",
    "            hit_items = set(recommended_posts).intersection(set(liked_posts))\n",
    "            hit = 1 if len(hit_items) > 0 else 0\n",
    "            precision = len(hit_items) / k\n",
    "            recall = len(hit_items) / len(liked_posts)\n",
    "            \n",
    "            metrics[k][\"hits\"] += hit\n",
    "            metrics[k][\"precision\"] += precision\n",
    "            metrics[k][\"recall\"] += recall\n",
    "            metrics[k][\"count\"] += 1\n",
    "\n",
    "# Print out the average metrics for each k value.\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "for k in k_list:\n",
    "    if metrics[k][\"count\"] > 0:\n",
    "        avg_hit = metrics[k][\"hits\"] / metrics[k][\"count\"]\n",
    "        avg_precision = metrics[k][\"precision\"] / metrics[k][\"count\"]\n",
    "        avg_recall = metrics[k][\"recall\"] / metrics[k][\"count\"]\n",
    "        print(f\"k = {k}: Hit Rate = {avg_hit:.3f}, Precision@{k} = {avg_precision:.3f}, Recall@{k} = {avg_recall:.3f}\")\n",
    "    else:\n",
    "        print(f\"k = {k}: No valid consumers were evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dims=4096\n",
    "# FAISS index built with 95110 posts.\n",
    "# Evaluating consumers: 100%|██████████| 15833/15833 [1:36:23<00:00,  2.74it/s]\n",
    "\n",
    "# Evaluation Metrics:\n",
    "# k = 20: Hit Rate = 0.926, Precision@20 = 0.287, Recall@20 = 0.721\n",
    "# k = 50: Hit Rate = 0.949, Precision@50 = 0.171, Recall@50 = 0.813\n",
    "# k = 100: Hit Rate = 0.963, Precision@100 = 0.110, Recall@100 = 0.872\n",
    "# k = 500: Hit Rate = 0.989, Precision@500 = 0.033, Recall@500 = 0.963\n",
    "# k = 1000: Hit Rate = 0.995, Precision@1000 = 0.018, Recall@1000 = 0.983"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
