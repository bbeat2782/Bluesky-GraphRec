{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleck/Projects/Bluesky-GraphRec/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>producer_idx</th>\n",
       "      <th>did</th>\n",
       "      <th>handle</th>\n",
       "      <th>display_name</th>\n",
       "      <th>description</th>\n",
       "      <th>followers</th>\n",
       "      <th>following</th>\n",
       "      <th>posts</th>\n",
       "      <th>joined</th>\n",
       "      <th>error</th>\n",
       "      <th>bsky_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>did:plc:nywb5oene54cllowkfwouxzz</td>\n",
       "      <td>chadloder.bsky.social</td>\n",
       "      <td>Chad Loder</td>\n",
       "      <td>Community activist, cybersecurity expert, citi...</td>\n",
       "      <td>54792.0</td>\n",
       "      <td>1377.0</td>\n",
       "      <td>10515.0</td>\n",
       "      <td>2023-04-30T18:10:33.161Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://bsky.app/profile/did:plc:nywb5oene54cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>did:plc:mnfnfpykrohxbck6av3f7los</td>\n",
       "      <td>blkmatters3000.bsky.social</td>\n",
       "      <td>Michael E Hopson</td>\n",
       "      <td>Organizer. Abolitionist \\nHe/him</td>\n",
       "      <td>134.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-05-01T22:40:48.536Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://bsky.app/profile/did:plc:mnfnfpykrohxb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>did:plc:kfdf3ncmu3ekd3yrorivypai</td>\n",
       "      <td>lolennui.bsky.social</td>\n",
       "      <td>Amy Ash</td>\n",
       "      <td>onion lady \\n\\nhttps://lolennui.com</td>\n",
       "      <td>52062.0</td>\n",
       "      <td>819.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>2023-04-27T15:52:02.340Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://bsky.app/profile/did:plc:kfdf3ncmu3ekd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>did:plc:rwbe4e7d7o3fwwcqkoyjvp4v</td>\n",
       "      <td>rui.bsky.social</td>\n",
       "      <td>Rui</td>\n",
       "      <td>None</td>\n",
       "      <td>5196.0</td>\n",
       "      <td>43075.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2023-04-11T18:05:18.291Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://bsky.app/profile/did:plc:rwbe4e7d7o3fw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>did:plc:krt7ulietkhjowpctmv2iphb</td>\n",
       "      <td>paulio.bsky.social</td>\n",
       "      <td>Paulio ðŸ¥š</td>\n",
       "      <td>None</td>\n",
       "      <td>1730.0</td>\n",
       "      <td>956.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>2023-05-02T02:45:41.097Z</td>\n",
       "      <td>None</td>\n",
       "      <td>https://bsky.app/profile/did:plc:krt7ulietkhjo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   producer_idx                               did                      handle  \\\n",
       "0             0  did:plc:nywb5oene54cllowkfwouxzz       chadloder.bsky.social   \n",
       "1             1  did:plc:mnfnfpykrohxbck6av3f7los  blkmatters3000.bsky.social   \n",
       "2             2  did:plc:kfdf3ncmu3ekd3yrorivypai        lolennui.bsky.social   \n",
       "3             3  did:plc:rwbe4e7d7o3fwwcqkoyjvp4v             rui.bsky.social   \n",
       "4             4  did:plc:krt7ulietkhjowpctmv2iphb          paulio.bsky.social   \n",
       "\n",
       "       display_name                                        description  \\\n",
       "0        Chad Loder  Community activist, cybersecurity expert, citi...   \n",
       "1  Michael E Hopson                   Organizer. Abolitionist \\nHe/him   \n",
       "2           Amy Ash                onion lady \\n\\nhttps://lolennui.com   \n",
       "3               Rui                                               None   \n",
       "4          Paulio ðŸ¥š                                               None   \n",
       "\n",
       "   followers  following    posts                    joined error  \\\n",
       "0    54792.0     1377.0  10515.0  2023-04-30T18:10:33.161Z  None   \n",
       "1      134.0       23.0      5.0  2023-05-01T22:40:48.536Z  None   \n",
       "2    52062.0      819.0   3329.0  2023-04-27T15:52:02.340Z  None   \n",
       "3     5196.0    43075.0     29.0  2023-04-11T18:05:18.291Z  None   \n",
       "4     1730.0      956.0   1138.0  2023-05-02T02:45:41.097Z  None   \n",
       "\n",
       "                                            bsky_url  \n",
       "0  https://bsky.app/profile/did:plc:nywb5oene54cl...  \n",
       "1  https://bsky.app/profile/did:plc:mnfnfpykrohxb...  \n",
       "2  https://bsky.app/profile/did:plc:kfdf3ncmu3ekd...  \n",
       "3  https://bsky.app/profile/did:plc:rwbe4e7d7o3fw...  \n",
       "4  https://bsky.app/profile/did:plc:krt7ulietkhjo...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import datamapplot\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "producer_embeddings = torch.load('../consumer-producer/producer_embeddings.pt', weights_only=False)\n",
    "producer_communities = np.load('../consumer-producer/producer_communities.npy')\n",
    "\n",
    "# Load 2D embeddings if they exist, otherwise create them\n",
    "try:\n",
    "    embeddings_2d = np.load('producer_embeddings_2d.npy')\n",
    "except FileNotFoundError:\n",
    "    # Reduce dimensionality to 2D using UMAP\n",
    "    reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "    embeddings_2d = reducer.fit_transform(producer_embeddings)\n",
    "    # Save the 2D embeddings\n",
    "    np.save('producer_embeddings_2d.npy', embeddings_2d)\n",
    "\n",
    "producer_df = pd.read_parquet('../consumer-producer/producer_profiles.parquet')\n",
    "producer_df['bsky_url'] = producer_df['did'].apply(lambda x: f\"https://bsky.app/profile/{x}\")\n",
    "producer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer_communities = producer_communities.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37192,), array(['49', '27', '24', ..., '14', '93', '87'], dtype='<U11'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "producer_communities.shape, producer_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0', '1', '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
       "       '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28',\n",
       "       '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38',\n",
       "       '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48',\n",
       "       '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58',\n",
       "       '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68',\n",
       "       '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78',\n",
       "       '79', '8', '80', '81', '82', '83', '84', '85', '86', '87', '88',\n",
       "       '89', '9', '90', '91', '92', '93', '94', '95', '96', '97', '98',\n",
       "       '99'], dtype='<U11')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(producer_communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create samples for each community\n",
    "import random\n",
    "import json\n",
    "\n",
    "# Create a dictionary mapping community to producer info\n",
    "community_samples = {}\n",
    "n_samples = 10  # number of samples per community\n",
    "\n",
    "# Add community as a column to producer_df for easier filtering\n",
    "producer_df['community'] = producer_communities\n",
    "\n",
    "for community in np.unique(producer_communities):\n",
    "    # Filter producers for this community\n",
    "    community_producers = producer_df[producer_df['community'] == community]\n",
    "    \n",
    "    # Sample n_samples or all if less than n_samples\n",
    "    sample_size = min(n_samples, len(community_producers))\n",
    "    sampled_producers = community_producers.sample(n=sample_size, random_state=42)\n",
    "    \n",
    "    # Store relevant information\n",
    "    community_samples[community] = {\n",
    "        'display_names': sampled_producers['display_name'].tolist(),\n",
    "        'handles': sampled_producers['handle'].tolist(),\n",
    "        'descriptions': sampled_producers['description'].fillna('').tolist()\n",
    "    }\n",
    "\n",
    "# Save the samples for later use\n",
    "with open('community_samples.json', 'w') as f:\n",
    "    json.dump(community_samples, f, indent=2)\n",
    "\n",
    "# Now we can proceed with the Gemma code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "Machines, with eyes that scan the land,\n",
      "Collect data, a treasure in the sand.\n",
      "Algorithms dance, a symphony of code,\n",
      "Unveiling patterns, a story to be told.\n",
      "\n",
      "From medical scans to weather's unpredictable turn,\n",
      "Machine learning's reach is ever unbound.\n",
      "It learns from mistakes, a constant evolution,\n",
      "A never-ending quest for knowledge and evolution.\n",
      "\n",
      "With each iteration, a new insight is found,\n",
      "A tapestry of data, a story profound.\n",
      "From mundane tasks to grandest of dreams,\n",
      "Machine learning's impact is ever apparent.\n",
      "\n",
      "But with power comes responsibility,\n",
      "A need for transparency, a moral responsibility.\n",
      "Bias and discrimination, a cautionary tale,\n",
      "Must be addressed with care and skill.\n",
      "\n",
      "So let us embrace this technology's might,\n",
      "To solve problems, to illuminate the night.\n",
      "But let us also remember, with every line of code,\n",
      "The human spirit, forever to be adored.<eos>\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating description for community 0...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'ids': 'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m community, profiles \u001b[38;5;129;01min\u001b[39;00m community_samples\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating description for community \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommunity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m     description \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_community_description\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     community_descriptions[community] \u001b[38;5;241m=\u001b[39m description\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommunity \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommunity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdescription\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[28], line 52\u001b[0m, in \u001b[0;36mgenerate_community_description\u001b[0;34m(profiles)\u001b[0m\n\u001b[1;32m     45\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m     49\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     50\u001b[0m )\n\u001b[0;32m---> 52\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRESPONSE\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Projects/Bluesky-GraphRec/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3851\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3848\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3849\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3852\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3853\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3854\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3855\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3856\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Bluesky-GraphRec/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:668\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(token_ids, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    667\u001b[0m     token_ids \u001b[38;5;241m=\u001b[39m [token_ids]\n\u001b[0;32m--> 668\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m clean_up_tokenization_spaces \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    671\u001b[0m     clean_up_tokenization_spaces\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    673\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclean_up_tokenization_spaces\n\u001b[1;32m    674\u001b[0m )\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clean_up_tokenization_spaces:\n",
      "\u001b[0;31mTypeError\u001b[0m: argument 'ids': 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/gemma-2b-it-bnb-4bit\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"unsloth/gemma-2b-it-bnb-4bit\")\n",
    "\n",
    "# Load the community samples\n",
    "with open('community_samples.json', 'r') as f:\n",
    "    community_samples = json.load(f)\n",
    "\n",
    "def generate_community_description(profiles):\n",
    "    # Create a prompt using the instruct format with an example\n",
    "    prompt = \"\"\"\n",
    "I will show you several social media profiles from the same community. Please provide a short description (1-3 words) that best describes this community. Return ONLY the description, no other text, no other words of any kind.\n",
    "\n",
    "Example:\n",
    "{\n",
    "Profiles:\n",
    "- CryptoTrader (@trader123)\n",
    "  Description: Full-time crypto trader. DeFi enthusiast. NFT collector. GM!\n",
    "- Alice (@alicecrypto)\n",
    "  Description: Building web3 infrastructure. ETH maxi. Love DeFi and DAOs.\n",
    "- Bob (@bobchain)\n",
    "  Description: Blockchain developer. Smart contract auditor. Crypto since 2015.\n",
    "\n",
    "Your response:\n",
    "defi & crypto\n",
    "}\n",
    "\n",
    "Profiles:\n",
    "\"\"\"\n",
    "    \n",
    "    # Add the actual profiles\n",
    "    for i in range(len(profiles['display_names'])):\n",
    "        display_name = profiles['display_names'][i]\n",
    "        handle = profiles['handles'][i]\n",
    "        description = profiles['descriptions'][i][:200]  # Truncate long descriptions\n",
    "        prompt += f\"- {display_name} (@{handle})\\n\"\n",
    "        prompt += f\"  Description: {description}\\n\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=10,\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    print(\"RESPONSE\", response)\n",
    "    return response\n",
    "\n",
    "# Generate descriptions for each community\n",
    "community_descriptions = {}\n",
    "for community, profiles in community_samples.items():\n",
    "    print(f\"Generating description for community {community}...\")\n",
    "    description = generate_community_description(profiles)\n",
    "    community_descriptions[community] = description\n",
    "    print(f\"Community {community}: {description}\\n\")\n",
    "\n",
    "\n",
    "# Save the descriptions\n",
    "with open('community_descriptions.json', 'w') as f:\n",
    "    json.dump(community_descriptions, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original embedding shape: (37192, 64)\n",
      "2D embedding shape: (37192, 2)\n",
      "Number of posts: 37192\n"
     ]
    }
   ],
   "source": [
    "hover_text_template = \"\"\"\n",
    "<div style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;\">\n",
    "    <p style=\"font-weight: 600; font-size: 14px; margin: 0 0 2px 0;\">{hover_text}</p>\n",
    "    <p style=\"font-size: 14px; color: #666; margin: 0 0 8px 0;\">@{handle}</p>\n",
    "    <p style=\"font-size: 14px; color: #4A4A4A; margin: 0 0 8px 0;\">{description}</p>\n",
    "    <div style=\"display: flex; gap: 16px; font-size: 13px; color: #666;\">\n",
    "        <span><b>{followers}</b> followers</span>\n",
    "        <span><b>{following}</b> following</span>\n",
    "        <span><b>{posts}</b> posts</span>\n",
    "    </div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Create the plot\n",
    "plot = datamapplot.create_interactive_plot(\n",
    "    embeddings_2d, \n",
    "    producer_communities,\n",
    "    hover_text=producer_df['display_name'].to_list(),\n",
    "    extra_point_data=producer_df[['handle','description', 'followers', 'following', 'bsky_url', 'posts']].fillna(''),\n",
    "    hover_text_html_template=hover_text_template,\n",
    "    on_click=\"window.open(hoverData.bsky_url[index], '_blank')\",\n",
    "    enable_search=True,\n",
    "    search_field=\"description\"\n",
    ")\n",
    "\n",
    "# Print some basic statistics about the embeddings\n",
    "print(f\"Original embedding shape: {producer_embeddings.shape}\")\n",
    "print(f\"2D embedding shape: {embeddings_2d.shape}\")\n",
    "print(f\"Number of posts: {len(producer_embeddings)}\")\n",
    "plot.save('producer_embeddings.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
