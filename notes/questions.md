why only use 70% of the training data? why only a subset of the data?

where exactly do the negative samples come from? from the entire train?

why model split into 2 parts?

can you explain the difference between transductive and inductive learning?

Where's the code for the date ranges?

where's the new code for the new user embeddings? 

you sample from the past 20 minutes? how much does that take in compute/storage? what exactly is the bottleneck?

why user dynamic features structured so weirdly? doesn't feel efficient.

when you call functions on the nodes, is it only on user nodes and not post nodes? or both? is the naming just so we can do multihop?

what are raw features like node_raw_features and edge_raw_features?

how long it takes to train using the current default params in readme?

which of the many functions are not used in the folders and I can ignore?

im confused on the flattening negative samples part and how exactly that is supposed to work.

still confused whether getting the temporal interactions are time based or quantity/id-based.